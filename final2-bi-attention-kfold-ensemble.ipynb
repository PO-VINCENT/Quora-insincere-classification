{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f5ea96dfcc682c232d6cbb6cebd8b48ecabf8fe"
   },
   "source": [
    "### Preface\n",
    "\n",
    "Hello . This is basically cutting and pasting from the amazing kernels of this competition. Please notify me if I don't attribute something correctly.\n",
    "\n",
    "* https://www.kaggle.com/gmhost/gru-capsule\n",
    "* How to: Preprocessing when using embeddings\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    "* Improve your Score with some Text Preprocessing https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing\n",
    "* Simple attention layer taken from https://github.com/mttk/rnn-classifier/blob/master/model.py\n",
    "* https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm\n",
    "* https://www.kaggle.com/hengzheng/pytorch-starter\n",
    "\n",
    "**UPDATE**: I seems that the shuffling the data doesn't add the features in the correct order. To address this issue I added a custom dataset class that can return indexes so that they can be accessed while training and properly put each feature with the corresponding sample. The training time though is increased, so you might need to make the model lighter in order to submit results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "710ed17d0c57bd287be0ee3b2782a53a54510561"
   },
   "source": [
    "## IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "abb7e3c30b8a412a50c6b451c49939e3cf4bc11b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Example\n",
    "from sklearn.metrics import f1_score\n",
    "import torchtext\n",
    "import os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a4ff5590a6f152dc1bec5aeca79aef10218f7de"
   },
   "source": [
    "### Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "deee49df5ca1c4413f71677939e26aa1ff784e44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use\n",
    "batch_size = 512 # how many samples to process at once\n",
    "#n_epochs = 4 # how many times to iterate over all samples\n",
    "#n_splits = 3 # Number of K-fold Splits\n",
    "\n",
    "SEED = 1029"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "654cbe3c8a1f2a618a2441afe00df3b4a89e0a58"
   },
   "source": [
    "### Ensure determinism in the results\n",
    "\n",
    "A common headache in this competition is the lack of determinism in the results due to cudnn. The following Kernel has a solution in Pytorch.\n",
    "\n",
    "See https://www.kaggle.com/hengzheng/pytorch-starter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "58bbf87335799247586aaed16531f4d28d10ed4a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c890692644acce2dc4f6e2f929d6d294faca4ad2"
   },
   "source": [
    "### Code for Loading Embeddings\n",
    "\n",
    "Functions taken from the kernel:https://www.kaggle.com/gmhost/gru-capsule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7026ee1d913f54dd4b560f654efdb9f833581cd3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## FUNCTIONS TAKEN FROM https://www.kaggle.com/gmhost/gru-capsule\n",
    "\n",
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea10c8e218a1280faa9802bcb7f1117c89ec96f9"
   },
   "source": [
    "## LOAD PROCESSED TRAINING DATA FROM DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "173753f0178464d2ba26baf22899884d76d1c83d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "df = pd.concat([df_train ,df_test],sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0f75559b6fa28c27ecbf145121309378afc884ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "vocab = build_vocab(df['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "5cb425ffbf1f79c1edc4cad3da15a1c1aa53edca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sincere questions: 1,225,312(93.81%) and # Insincere questions: 80,810(6.19%)\n",
      "# Test samples: 375,806(0.288% of train samples)\n"
     ]
    }
   ],
   "source": [
    "sin = len(df_train[df_train[\"target\"]==0])\n",
    "insin = len(df_train[df_train[\"target\"]==1])\n",
    "persin = (sin/(sin+insin))*100\n",
    "perinsin = (insin/(sin+insin))*100            \n",
    "print(\"# Sincere questions: {:,}({:.2f}%) and # Insincere questions: {:,}({:.2f}%)\".format(sin,persin,insin,perinsin))\n",
    "# print(\"Sinsere:{}% Insincere: {}%\".format(round(persin,2),round(perinsin,2)))\n",
    "print(\"# Test samples: {:,}({:.3f}% of train samples)\".format(len(df_test),len(df_test)/len(df_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07e9890ec0b490cef57565f7dff953aa56ebd3dc"
   },
   "source": [
    "\n",
    "## Normalization\n",
    "\n",
    "Borrowed from:\n",
    "* How to: Preprocessing when using embeddings\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    "* Improve your Score with some Text Preprocessing https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "28ebb28ba78972bb8d4fee9b53437045542d20fb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "abeab4c80d6829cf2eae706bfa7929e2871af81f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c09d981ae674e6a373189a04dba8d0932b0765b"
   },
   "source": [
    "Extra feature part taken from https://github.com/wongchunghang/toxic-comment-challenge-lstm/blob/master/toxic_comment_9872_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "63cb21525251b060aeb309e7be4b48772f8720f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    \n",
    "    df['question_text'] = df['question_text'].progress_apply(lambda x:str(x))\n",
    "    df['total_length'] = df['question_text'].progress_apply(len)\n",
    "    df['capitals'] = df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.progress_apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['question_text'].progress_apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(\"../input/train.csv\")\n",
    "    test_df = pd.read_csv(\"../input/test.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    # lower\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean the text\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    # Clean numbers\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    # Clean speelings\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "\n",
    "    \n",
    "    ###################### Add Features ###############################\n",
    "    #  https://github.com/wongchunghang/toxic-comment-challenge-lstm/blob/master/toxic_comment_9872_model.ipynb\n",
    "    train = add_features(train_df)\n",
    "    test = add_features(test_df)\n",
    "\n",
    "    features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "    test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((features, test_features)))\n",
    "    features = ss.transform(features)\n",
    "    test_features = ss.transform(test_features)\n",
    "    ###########################################################################\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "#     # Splitting to training and a final test set    \n",
    "#     train_X, x_test_f, train_y, y_test_f = train_test_split(list(zip(train_X,features)), train_y, test_size=0.2, random_state=SEED)    \n",
    "#     train_X, features = zip(*train_X)\n",
    "#     x_test_f, features_t = zip(*x_test_f)    \n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(SEED)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    features = features[trn_idx]\n",
    "    \n",
    "    return train_X, test_X, train_y, features, test_features, tokenizer.word_index\n",
    "#     return train_X, test_X, train_y, x_test_f,y_test_f,features, test_features, features_t, tokenizer.word_index\n",
    "#     return train_X, test_X, train_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "3c72fcddb4f680879e231c3dbfc0c71e27fc424c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f165443fb042829793cdf7e70eb98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cded09ff44e48e4b2a0dd44c91b5616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc08f60c7ef4787a489c4668f9dd7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356826fed5714418888571d8744972e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f8da8718cc456db94cb4660493f358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dade08d46945ceb100ded718f3e23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af396f40d3254767b23e66081f3bfbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33e170051da4b2087c08327382f7b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ac612e6822428eb9e569d5126d2d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a4705d72be48eab3d8c58c75f19363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30230dd15424193be84b61e7a4c0f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff5a4654015410ea91928154455384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6e1464131541109c42ddeb7df9c5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=375806, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fill up the missing values\n",
    "# x_train, x_test, y_train, word_index = load_and_prec()\n",
    "x_train, x_test, y_train, features, test_features, word_index = load_and_prec() \n",
    "# x_train, x_test, y_train, x_test_f,y_test_f,features, test_features,features_t, word_index = load_and_prec() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5264b1a511613e0cb9cffc11e93e906f932be191"
   },
   "source": [
    "### SAVE DATASET TO DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "7dc2704001695c0d691fec74cc111308da3fc340",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(\"x_train\",x_train)\n",
    "np.save(\"x_test\",x_test)\n",
    "np.save(\"y_train\",y_train)\n",
    "\n",
    "np.save(\"features\",features)\n",
    "np.save(\"test_features\",test_features)\n",
    "np.save(\"word_index.npy\",word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef622350c3ac00bcbea516ccf7dffbeca7b4cc39"
   },
   "source": [
    "### LOAD DATASET FROM DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "6e1f33f0d744e86cfbcc21b8e696bb568ba1c454",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = np.load(\"x_train.npy\")\n",
    "x_test = np.load(\"x_test.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "features = np.load(\"features.npy\")\n",
    "test_features = np.load(\"test_features.npy\")\n",
    "word_index = np.load(\"word_index.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "6db7b1d3f22a6e35699d9f0dbb51343e249d9ee7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306122, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5c51a8329d569d13b9f0369ebb98ca8e2e55440"
   },
   "source": [
    "### Load Embeddings\n",
    "\n",
    "Two embedding matrices have been used. Glove, and paragram. The mean of the two is used as the final embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "6a5f4502324d369ff6faa3692accee4f8a233005",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:46: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing entries in the embedding are set using np.random.normal so we have to seed here too\n",
    "seed_everything()\n",
    "\n",
    "glove_embeddings = load_glove(word_index)\n",
    "paragram_embeddings = load_para(word_index)\n",
    "\n",
    "embedding_matrix = np.mean((1.28*glove_embeddings, 0.72*paragram_embeddings), axis=0)\n",
    "\n",
    "# vocab = build_vocab(df['question_text'])\n",
    "# add_lower(embedding_matrix, vocab)\n",
    "del glove_embeddings, paragram_embeddings\n",
    "gc.collect()\n",
    "\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "2484f29f159af679af7d7745e522221e31e42ce0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8098bea0cee9117ff9dc4e11feba53e49b80cb55"
   },
   "source": [
    "### Cyclic CLR\n",
    "Code taken from https://www.kaggle.com/dannykliu/lstm-with-attention-clr-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "9d531a7454923f90d0e7443b1ed1373d008c2e88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code inspired from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8caecb207a12d4a5524fe16e4524b31c7da8bac"
   },
   "source": [
    "### model1\n",
    "\n",
    "Binary LSTM with an attention layer and an additional fully connected layer. Also added extra features taken from a winning kernel of the toxic comments competition. Also using CLR and a capsule Layer. Blended together in concatentation.\n",
    "\n",
    "Initial idea borrowed from: https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "8f7e1d15451201efbac2741b03d4b0dfd3b9bfe0"
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_path = '../save/embedding_matrix.npy'  # or False, not use pre-trained-matrix\n",
    "use_pretrained_embedding = True\n",
    "\n",
    "hidden_size = 60\n",
    "gru_len = hidden_size\n",
    "\n",
    "Routings = 4 #5\n",
    "Num_capsule = 5\n",
    "Dim_capsule = 5#16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "LR = 0.001\n",
    "T_epsilon = 1e-7\n",
    "num_classes = 30\n",
    "\n",
    "\n",
    "class Embed_Layer(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None, embedding_dim=300):\n",
    "        super(Embed_Layer, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        if use_pretrained_embedding:\n",
    "            # self.encoder.weight.data.copy_(t.from_numpy(np.load(embedding_path))) # 方法一，加载np.save的npy文件\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(embedding_matrix))  # 方法二\n",
    "\n",
    "    def forward(self, x, dropout_p=0.25):\n",
    "        return nn.Dropout(p=dropout_p)(self.encoder(x))\n",
    "\n",
    "\n",
    "class GRU_Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU_Layer, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=300,\n",
    "                          hidden_size=gru_len,\n",
    "                          bidirectional=True)\n",
    "        '''\n",
    "        自己修改GRU里面的激活函数及加dropout和recurrent_dropout\n",
    "        如果要使用，把rnn_revised import进来，但好像是使用cpu跑的，比较慢\n",
    "       '''\n",
    "        # # if you uncomment /*from rnn_revised import * */, uncomment following code aswell\n",
    "        # self.gru = RNNHardSigmoid('GRU', input_size=300,\n",
    "        #                           hidden_size=gru_len,\n",
    "        #                           bidirectional=True)\n",
    "\n",
    "    # 这步很关键，需要像keras一样用glorot_uniform和orthogonal_uniform初始化参数\n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gru(x)\n",
    "\n",
    "\n",
    "# core caps_layer with squash func\n",
    "class Caps_Layer(nn.Module):\n",
    "    def __init__(self, input_dim_capsule=gru_len * 2, num_capsule=Num_capsule, dim_capsule=Dim_capsule, \\\n",
    "                 routings=Routings, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Caps_Layer, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size  # 暂时没用到\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = self.squash\n",
    "        else:\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        if self.share_weights:\n",
    "            self.W = nn.Parameter(\n",
    "                nn.init.xavier_normal_(t.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "        else:\n",
    "            self.W = nn.Parameter(\n",
    "                t.randn(BATCH_SIZE, input_dim_capsule, self.num_capsule * self.dim_capsule))  # 64即batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = t.matmul(x, self.W)\n",
    "        else:\n",
    "            print('add later')\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        input_num_capsule = x.size(1)\n",
    "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                      self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)  # 转成(batch_size,num_capsule,input_num_capsule,dim_capsule)\n",
    "        b = t.zeros_like(u_hat_vecs[:, :, :, 0])  # (batch_size,num_capsule,input_num_capsule)\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            b = b.permute(0, 2, 1)\n",
    "            c = F.softmax(b, dim=2)\n",
    "            c = c.permute(0, 2, 1)\n",
    "            b = b.permute(0, 2, 1)\n",
    "            outputs = self.activation(t.einsum('bij,bijk->bik', (c, u_hat_vecs)))  # batch matrix multiplication\n",
    "            # outputs shape (batch_size, num_capsule, dim_capsule)\n",
    "            if i < self.routings - 1:\n",
    "                b = t.einsum('bik,bijk->bij', (outputs, u_hat_vecs))  # batch matrix multiplication\n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    # text version of squash, slight different from original one\n",
    "    def squash(self, x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = t.sqrt(s_squared_norm + T_epsilon)\n",
    "        return x / scale\n",
    "    \n",
    "class Capsule_Main(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None):\n",
    "        super(Capsule_Main, self).__init__()\n",
    "        self.embed_layer = Embed_Layer(embedding_matrix, vocab_size)\n",
    "        self.gru_layer = GRU_Layer()\n",
    "        # 【重要】初始化GRU权重操作，这一步非常关键，acc上升到0.98，如果用默认的uniform初始化则acc一直在0.5左右\n",
    "        self.gru_layer.init_weights()\n",
    "        self.caps_layer = Caps_Layer()\n",
    "        self.dense_layer = Dense_Layer()\n",
    "\n",
    "    def forward(self, content):\n",
    "        content1 = self.embed_layer(content)\n",
    "        content2, _ = self.gru_layer(\n",
    "            content1)  # 这个输出是个tuple，一个output(seq_len, batch_size, num_directions * hidden_size)，一个hn\n",
    "        content3 = self.caps_layer(content2)\n",
    "        output = self.dense_layer(content3)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "170b27f66c7e546d8e84c79357d40f86c6b1ec42",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        fc_layer = 16\n",
    "        fc_layer1 = 16\n",
    "\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.bn = nn.BatchNorm1d(16, momentum=0.5)\n",
    "        self.linear = nn.Linear(hidden_size*8+3, fc_layer1) #643:80 - 483:60 - 323:40\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(fc_layer**2,fc_layer)\n",
    "        self.out = nn.Linear(fc_layer, 1)\n",
    "        self.lincaps = nn.Linear(Num_capsule * Dim_capsule, 1)\n",
    "        self.caps_layer = Caps_Layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "#         Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "\n",
    "        h_embedding = self.embedding(x[0])\n",
    "        h_embedding = torch.squeeze(\n",
    "            self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "\n",
    "        ##Capsule Layer        \n",
    "        content3 = self.caps_layer(h_gru)\n",
    "        content3 = self.dropout(content3)\n",
    "        batch_size = content3.size(0)\n",
    "        content3 = content3.view(batch_size, -1)\n",
    "        content3 = self.relu(self.lincaps(content3))\n",
    "\n",
    "        ##Attention Layer\n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()\n",
    "\n",
    "                #[512,160]\n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten,content3, avg_pool, max_pool,f), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.bn(conc)\n",
    "        conc = self.dropout(conc)\n",
    "\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb12d5409bd896899737aed7953127f2ef21bbc7"
   },
   "source": [
    "### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "0ce1356e2fbb03a3a51e1147898afa31116d2494"
   },
   "outputs": [],
   "source": [
    "class Alex_NeuralNet_Meta(nn.Module):\n",
    "    def __init__(self,hidden_size,lin_size,embedding_matrix=embedding_matrix):\n",
    "        super(Alex_NeuralNet_Meta, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        drp = 0.1\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        for name, param in self.gru.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size*6 + features.shape[1], lin_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drp)\n",
    "        self.out = nn.Linear(lin_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x[0])\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        #print(\"emb\", h_embedding.size())\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        #print(\"lst\",h_lstm.size())\n",
    "        h_gru, hh_gru = self.gru(h_lstm)\n",
    "        hh_gru = hh_gru.view(-1, 2*self.hidden_size )\n",
    "        #print(\"gru\", h_gru.size())\n",
    "        #print(\"h_gru\", hh_gru.size())\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        #print(\"avg_pool\", avg_pool.size())\n",
    "        #print(\"max_pool\", max_pool.size())\n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()\n",
    "        #print(\"f\", f.size())\n",
    "        conc = torch.cat(( hh_gru, avg_pool, max_pool,f), 1)\n",
    "        #print(\"conc\", conc.size())\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a06d30e0195e788b7638a6c545d7c6ff7d452e4"
   },
   "source": [
    "**model3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "8512ef38748ea79a1c9df69f7b297ab02a7f55dc"
   },
   "outputs": [],
   "source": [
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        \n",
    "        hidden_size = 40\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.gru_1 = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru_2 = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.attention = Attention(hidden_size*2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*2, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_gru_1, _ = self.gru_1(h_embedding)\n",
    "        h_gru_2, _ = self.gru_2(h_gru_1)\n",
    "        \n",
    "        h_atten = self.attention(h_gru_2)\n",
    "        \n",
    "        fc = self.relu(self.linear(h_atten))\n",
    "        fc = self.dropout(fc)\n",
    "        out = self.out(fc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6840f09766333b1608100a8f4727437905e7c7ad"
   },
   "source": [
    "**model4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "b8fed3ad2cd8995790b947a111549f541b76483a"
   },
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        \n",
    "        hidden_size = 40\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.attention = Attention(hidden_size*2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*6, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_atten = self.attention(h_lstm)\n",
    "        \n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        conc = torch.cat((h_atten, avg_pool, max_pool), 1)\n",
    "        \n",
    "        fc = self.relu(self.linear(conc))\n",
    "        fc = self.dropout(fc)\n",
    "        out = self.out(fc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c863bc2202cb4e04ff304c2eb933844202a69c2a"
   },
   "source": [
    "**model5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "fbb5acfecd1c13bb58d94b4d644a930281fd8f31"
   },
   "outputs": [],
   "source": [
    "class NeuralNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet3, self).__init__()\n",
    "        \n",
    "        hidden_size = 40\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size*2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size*2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*8, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "9ffd056b953b0549820bfb17a9c091168f23729c"
   },
   "outputs": [],
   "source": [
    "model1 = NeuralNet()\n",
    "model2 = Alex_NeuralNet_Meta(70,16, embedding_matrix=embedding_matrix)\n",
    "model3 = NeuralNet1()\n",
    "model4 = NeuralNet2()\n",
    "model5 = NeuralNet3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "9b4dc3474970ac24fcbb0499cebc4d9245fe58d3"
   },
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay') # decoupled weight decay (2/4)\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    #@interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p # decoupled weight decay (4/4)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4e47597cde552a41cdd8ec2531aa6a861e491ae"
   },
   "source": [
    "### Training\n",
    "\n",
    "The method for training is borrowed from https://www.kaggle.com/hengzheng/pytorch-starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "a77141e23d52fe8035c671c19aeb82e850fe0441"
   },
   "outputs": [],
   "source": [
    "def bestThresshold(y_train,train_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "704fcb82f5f25d6349a487409f519384df352375"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "5e0f0741b5a7787d89eadd48fe5321bb70450206"
   },
   "outputs": [],
   "source": [
    "def pytorch_model_run_cv(n_splits,n_epochs,x_train,y_train,features ,x_test, model_obj, clip = True):\n",
    "    seed_everything()\n",
    "    avg_losses_f = []\n",
    "    avg_val_losses_f = []\n",
    "    # matrix for the out-of-fold predictions\n",
    "    train_preds = np.zeros((len(x_train)))\n",
    "    # matrix for the predictions on the test set\n",
    "    test_preds = np.zeros((len(x_test)))\n",
    "    x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "    \n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        seed_everything(i*1000+i)\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        \n",
    "        features = np.array(features)\n",
    "        x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda()\n",
    "        y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "        \n",
    "        kfold_X_features = features[train_idx.astype(int)]\n",
    "        kfold_X_valid_features = features[valid_idx.astype(int)]\n",
    "        x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda()\n",
    "        y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "        \n",
    "        model = copy.deepcopy(model_obj)\n",
    "\n",
    "        model.cuda()\n",
    "\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "        step_size = 300\n",
    "        base_lr, max_lr = 0.001, 0.003   \n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                                 lr=max_lr)\n",
    "        #optimizer = AdamW(weight_decay=0.07)\n",
    "        \n",
    "        ################################################################################################\n",
    "        scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "                   step_size=step_size, mode='exp_range',\n",
    "                   gamma=0.99994)\n",
    "        ###############################################################################################\n",
    "\n",
    "        train = MyDataset(torch.utils.data.TensorDataset(x_train_fold, y_train_fold))\n",
    "        valid = MyDataset(torch.utils.data.TensorDataset(x_val_fold, y_val_fold))\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        print(f'Fold {i + 1}')\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "\n",
    "            avg_loss = 0.  \n",
    "            for i, (x_batch, y_batch, index) in enumerate(train_loader):      \n",
    "                f = kfold_X_features[index]\n",
    "                y_pred = model([x_batch,f])\n",
    "                \n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.batch_step()\n",
    "\n",
    "                # Compute and print loss.\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                if clip:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(),1)\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "                \n",
    "            model.eval()\n",
    "            \n",
    "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "            test_preds_fold = np.zeros((len(x_test)))\n",
    "            \n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch,index) in enumerate(valid_loader):\n",
    "                f = kfold_X_valid_features[index]            \n",
    "                y_pred = model([x_batch,f]).detach()\n",
    "                \n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds_fold[index] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            \n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        avg_losses_f.append(avg_loss)\n",
    "        avg_val_losses_f.append(avg_val_loss) \n",
    "        # predict all samples in the test set batch per batch\n",
    "        for i, (x_batch,) in enumerate(test_loader):\n",
    "            f = test_features[i * batch_size:(i+1) * batch_size]\n",
    "            y_pred = model([x_batch,f]).detach()\n",
    "            test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            \n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "    print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))\n",
    "    return train_preds, test_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "3930c16435b956ad2f9775056e6bb46ee94bdc1e"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "6af3c7c6ca157f049426cabac646461a95cc2ac6"
   },
   "outputs": [],
   "source": [
    "def stacking_level1(x_train, y_train, test):\n",
    "    \"\"\" stacking\n",
    "    input:  train_x, train_y, test\n",
    "    output: test的预测值\n",
    "    clfs:   5个一级classifier\n",
    "    dataset_blend_train: 一级分类器的prediction, 二级分类器的train_x\n",
    "    dataset_blend_test: 二级分类器的test\n",
    "    \"\"\"\n",
    "    # 5 个一级分类器\n",
    "    #models = [model1, model2, model3, model4, model5]\n",
    "    models = [model1, model2]\n",
    "    # 二级分类器的 train_x, test\n",
    "    dataset_blend_train = np.zeros((x_train.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_test.shape[0],len(models)))\n",
    "    # x_test_cuda_f = torch.tensor(x_test_f, dtype=torch.long).cuda()\n",
    "    # test_f = torch.utils.data.TensorDataset(x_test_cuda_f)\n",
    "    # test_loader_f = torch.utils.data.DataLoader(test_f, batch_size=batch_size, shuffle=False)\n",
    "    x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # 5个分类器进行4 folds 的预测\n",
    "    \n",
    "    # splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    #for i, model in enumerate(models):\n",
    "        # dataset_blend_test = np.zeros((x_test.shape[0],n_splits)) # 每个分类器单次fold的预测结果\n",
    "        # always call this before training for deterministic results\n",
    "    print(f'Model 1')\n",
    "    start_time = time.time()\n",
    "    seed_everything()\n",
    "    train_preds, test_preds = pytorch_model_run_cv(3,4,x_train,y_train,features,x_test,models[0], clip = True)\n",
    "        #print(train_preds[:10])\n",
    "        #print(test_preds[:10])\n",
    "    dataset_blend_train[:,0] = train_preds \n",
    "    dataset_blend_test[:,0]= test_preds\n",
    "        \n",
    "    delta = bestThresshold(y_train,train_preds)\n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('Model 1 \\t time={:.2f}s'.format( elapsed_time))\n",
    "        \n",
    "        \n",
    "    print(f'Model 2')\n",
    "    start_time = time.time()\n",
    "    seed_everything()\n",
    "    train_preds, test_preds = pytorch_model_run_cv(4,4,x_train,y_train,features,x_test,models[1], clip = True)\n",
    "        #print(train_preds[:10])\n",
    "        #print(test_preds[:10])\n",
    "    dataset_blend_train[:,1] = train_preds \n",
    "    dataset_blend_test[:,1]= test_preds\n",
    "        \n",
    "    delta = bestThresshold(y_train,train_preds)\n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('Model 2\\t time={:.2f}s'.format(elapsed_time))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset_blend_train\n",
    "    dataset_blend_test \n",
    "    print(dataset_blend_train.shape)\n",
    "    print(dataset_blend_train[:10])\n",
    "    print(dataset_blend_test.shape)\n",
    "    print(dataset_blend_test[:10])\n",
    "    \n",
    "    return dataset_blend_train, dataset_blend_test\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "7655676390a85eefc5530b31e0f74f8d6b26bdbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Fold 1\n",
      "Epoch 1/4 \t loss=82.6775 \t val_loss=83.3541 \t time=228.78s\n",
      "Epoch 2/4 \t loss=58.0428 \t val_loss=73.3002 \t time=228.71s\n",
      "Epoch 3/4 \t loss=54.6977 \t val_loss=69.2202 \t time=229.12s\n",
      "Epoch 4/4 \t loss=51.8260 \t val_loss=63.8921 \t time=228.20s\n",
      "Fold 2\n",
      "Epoch 1/4 \t loss=82.8269 \t val_loss=53.0296 \t time=228.52s\n",
      "Epoch 2/4 \t loss=57.9660 \t val_loss=51.8058 \t time=228.95s\n",
      "Epoch 3/4 \t loss=54.5343 \t val_loss=50.9174 \t time=230.04s\n",
      "Epoch 4/4 \t loss=51.5539 \t val_loss=51.8708 \t time=229.95s\n",
      "Fold 3\n",
      "Epoch 1/4 \t loss=82.5439 \t val_loss=56.9000 \t time=229.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 \t loss=57.9172 \t val_loss=53.1301 \t time=229.12s\n",
      "Epoch 3/4 \t loss=54.6814 \t val_loss=50.6996 \t time=228.82s\n",
      "Epoch 4/4 \t loss=51.3871 \t val_loss=51.6587 \t time=227.75s\n",
      "All \t loss=51.5890 \t val_loss=55.8072 \t \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6274869e428d472a9177f8428dcd074e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best threshold is 0.3000 with F1 score: 0.6804\n",
      "Model 1 \t time=2848.54s\n",
      "Model 2\n",
      "Fold 1\n",
      "Epoch 1/4 \t loss=66.7233 \t val_loss=53.0718 \t time=187.46s\n",
      "Epoch 2/4 \t loss=58.8743 \t val_loss=51.1370 \t time=187.90s\n",
      "Epoch 3/4 \t loss=55.7077 \t val_loss=50.3327 \t time=190.41s\n",
      "Epoch 4/4 \t loss=52.8582 \t val_loss=51.3938 \t time=190.51s\n",
      "Fold 2\n",
      "Epoch 1/4 \t loss=66.2365 \t val_loss=55.4370 \t time=190.66s\n",
      "Epoch 2/4 \t loss=58.8262 \t val_loss=52.4290 \t time=190.45s\n",
      "Epoch 3/4 \t loss=55.5891 \t val_loss=52.2097 \t time=190.35s\n",
      "Epoch 4/4 \t loss=52.4784 \t val_loss=51.8494 \t time=190.07s\n",
      "Fold 3\n",
      "Epoch 1/4 \t loss=66.2595 \t val_loss=53.2045 \t time=190.64s\n",
      "Epoch 2/4 \t loss=58.6778 \t val_loss=51.8308 \t time=190.39s\n",
      "Epoch 3/4 \t loss=55.5037 \t val_loss=51.6458 \t time=190.88s\n",
      "Epoch 4/4 \t loss=52.4515 \t val_loss=54.6944 \t time=190.52s\n",
      "Fold 4\n",
      "Epoch 1/4 \t loss=66.7312 \t val_loss=53.5437 \t time=190.91s\n",
      "Epoch 2/4 \t loss=58.3578 \t val_loss=52.0369 \t time=190.72s\n",
      "Epoch 3/4 \t loss=54.9133 \t val_loss=50.3826 \t time=190.69s\n",
      "Epoch 4/4 \t loss=51.9181 \t val_loss=52.0872 \t time=190.49s\n",
      "All \t loss=52.4266 \t val_loss=52.5062 \t \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016643f6d1c5482cbdd062bd6b6ced8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best threshold is 0.2700 with F1 score: 0.6804\n",
      "Model 2\t time=3138.09s\n",
      "(1306122, 2)\n",
      "[[4.14315145e-03 7.47067332e-02]\n",
      " [3.80451865e-02 1.19980434e-02]\n",
      " [1.61285943e-03 1.26219774e-03]\n",
      " [2.53595531e-01 7.62969479e-02]\n",
      " [2.38915090e-03 7.17676338e-03]\n",
      " [2.77974497e-04 3.44463653e-04]\n",
      " [2.67606047e-05 1.95879729e-05]\n",
      " [8.19432316e-05 9.40628015e-05]\n",
      " [5.11797902e-04 1.23291102e-03]\n",
      " [1.40852365e-03 4.71675186e-04]]\n",
      "(375806, 2)\n",
      "[[8.45237235e-01 7.21422285e-01]\n",
      " [1.40485203e-04 2.62656064e-04]\n",
      " [5.02537989e-04 1.09998567e-04]\n",
      " [9.00590466e-03 2.94451418e-03]\n",
      " [4.27758696e-03 3.09834616e-03]\n",
      " [3.50029953e-03 1.83356948e-03]\n",
      " [5.29508992e-05 3.24702357e-05]\n",
      " [3.85815472e-04 1.40634605e-04]\n",
      " [4.17940447e-03 6.53863084e-04]\n",
      " [3.29878889e-04 1.23576559e-04]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dataset_blend_train, dataset_blend_test = stacking_level1(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "af75d1e9b873f94f41588ba84bc377ada00364f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306122, 2)\n",
      "(375806, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_blend_train.shape)\n",
    "print(dataset_blend_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "f24de9673891731ef8c2810bbe9a2a136d3a2701"
   },
   "outputs": [],
   "source": [
    "def stacking_level2(x_train,x_test,y_train):\n",
    "    #二级分类器进行预测\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    splits = list(StratifiedKFold(n_splits=n_splits1, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    \n",
    "    for i,[train_idx, valid_idx] in enumerate (splits):\n",
    "        print(f'2nd level Fold {i + 1}')\n",
    "        x_train_tr = x_train[train_idx]\n",
    "        y_train_tr = y_train[train_idx]\n",
    "        \n",
    "        x_train_val = x_train[valid_idx]\n",
    "        y_train_val = y_train[valid_idx]\n",
    "    \n",
    "        d_train = lgb.Dataset(x_train_tr,y_train_tr)\n",
    "        d_val = lgb.Dataset(x_train_val,y_train_val)\n",
    "        params = {}\n",
    "        params['learning_rate'] = 0.02\n",
    "        params['boosting_type'] = 'gbdt'\n",
    "        params['objective'] = 'binary'\n",
    "        params['metric'] = 'binary_logloss'\n",
    "        params['sub_feature'] = 0.5\n",
    "        params['num_leaves'] = 80\n",
    "        params['min_data'] = 500\n",
    "        params['max_depth'] = 10\n",
    "        params['lambda_l1']= 0.1\n",
    "        \n",
    "        clf = lgb.train(params, d_train, 500, valid_sets = d_val, early_stopping_rounds=20)\n",
    "        \n",
    "        y_pred_val = clf.predict(x_train_val)\n",
    "        \n",
    "    delta = bestThresshold(y_train_val,y_pred_val)\n",
    "        \n",
    "        # f1_score= f1_score(y_train_val,y_pred_val) \n",
    "        # print('f1_score',f1_score)\n",
    "        \n",
    "    y_pred_test = clf.predict(x_test)\n",
    "    prediction = (y_pred_test>delta).astype(int)\n",
    "         \n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('2nd prediction \\t time={:.2f}s'.format(elapsed_time))\n",
    "    \n",
    "    return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "f4990ccffc38622672cd12a1ea90e79ac31d927a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd level Fold 1\n",
      "[1]\tvalid_0's binary_logloss: 0.2214\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.213133\n",
      "[3]\tvalid_0's binary_logloss: 0.205927\n",
      "[4]\tvalid_0's binary_logloss: 0.199858\n",
      "[5]\tvalid_0's binary_logloss: 0.19436\n",
      "[6]\tvalid_0's binary_logloss: 0.18954\n",
      "[7]\tvalid_0's binary_logloss: 0.18508\n",
      "[8]\tvalid_0's binary_logloss: 0.181082\n",
      "[9]\tvalid_0's binary_logloss: 0.177336\n",
      "[10]\tvalid_0's binary_logloss: 0.173925\n",
      "[11]\tvalid_0's binary_logloss: 0.170697\n",
      "[12]\tvalid_0's binary_logloss: 0.16773\n",
      "[13]\tvalid_0's binary_logloss: 0.164903\n",
      "[14]\tvalid_0's binary_logloss: 0.162286\n",
      "[15]\tvalid_0's binary_logloss: 0.159776\n",
      "[16]\tvalid_0's binary_logloss: 0.157442\n",
      "[17]\tvalid_0's binary_logloss: 0.155194\n",
      "[18]\tvalid_0's binary_logloss: 0.153095\n",
      "[19]\tvalid_0's binary_logloss: 0.151066\n",
      "[20]\tvalid_0's binary_logloss: 0.149166\n",
      "[21]\tvalid_0's binary_logloss: 0.147323\n",
      "[22]\tvalid_0's binary_logloss: 0.145592\n",
      "[23]\tvalid_0's binary_logloss: 0.143909\n",
      "[24]\tvalid_0's binary_logloss: 0.142325\n",
      "[25]\tvalid_0's binary_logloss: 0.140783\n",
      "[26]\tvalid_0's binary_logloss: 0.139328\n",
      "[27]\tvalid_0's binary_logloss: 0.137908\n",
      "[28]\tvalid_0's binary_logloss: 0.136568\n",
      "[29]\tvalid_0's binary_logloss: 0.135257\n",
      "[30]\tvalid_0's binary_logloss: 0.134018\n",
      "[31]\tvalid_0's binary_logloss: 0.132805\n",
      "[32]\tvalid_0's binary_logloss: 0.131657\n",
      "[33]\tvalid_0's binary_logloss: 0.130531\n",
      "[34]\tvalid_0's binary_logloss: 0.129465\n",
      "[35]\tvalid_0's binary_logloss: 0.128419\n",
      "[36]\tvalid_0's binary_logloss: 0.127427\n",
      "[37]\tvalid_0's binary_logloss: 0.126453\n",
      "[38]\tvalid_0's binary_logloss: 0.125529\n",
      "[39]\tvalid_0's binary_logloss: 0.12462\n",
      "[40]\tvalid_0's binary_logloss: 0.123757\n",
      "[41]\tvalid_0's binary_logloss: 0.122908\n",
      "[42]\tvalid_0's binary_logloss: 0.122102\n",
      "[43]\tvalid_0's binary_logloss: 0.121308\n",
      "[44]\tvalid_0's binary_logloss: 0.120554\n",
      "[45]\tvalid_0's binary_logloss: 0.119811\n",
      "[46]\tvalid_0's binary_logloss: 0.119104\n",
      "[47]\tvalid_0's binary_logloss: 0.118408\n",
      "[48]\tvalid_0's binary_logloss: 0.117746\n",
      "[49]\tvalid_0's binary_logloss: 0.117092\n",
      "[50]\tvalid_0's binary_logloss: 0.116471\n",
      "[51]\tvalid_0's binary_logloss: 0.115857\n",
      "[52]\tvalid_0's binary_logloss: 0.115274\n",
      "[53]\tvalid_0's binary_logloss: 0.114698\n",
      "[54]\tvalid_0's binary_logloss: 0.11415\n",
      "[55]\tvalid_0's binary_logloss: 0.113608\n",
      "[56]\tvalid_0's binary_logloss: 0.113093\n",
      "[57]\tvalid_0's binary_logloss: 0.112583\n",
      "[58]\tvalid_0's binary_logloss: 0.112098\n",
      "[59]\tvalid_0's binary_logloss: 0.111619\n",
      "[60]\tvalid_0's binary_logloss: 0.111162\n",
      "[61]\tvalid_0's binary_logloss: 0.110711\n",
      "[62]\tvalid_0's binary_logloss: 0.110281\n",
      "[63]\tvalid_0's binary_logloss: 0.109856\n",
      "[64]\tvalid_0's binary_logloss: 0.109451\n",
      "[65]\tvalid_0's binary_logloss: 0.10905\n",
      "[66]\tvalid_0's binary_logloss: 0.108668\n",
      "[67]\tvalid_0's binary_logloss: 0.10829\n",
      "[68]\tvalid_0's binary_logloss: 0.10793\n",
      "[69]\tvalid_0's binary_logloss: 0.107574\n",
      "[70]\tvalid_0's binary_logloss: 0.107235\n",
      "[71]\tvalid_0's binary_logloss: 0.106899\n",
      "[72]\tvalid_0's binary_logloss: 0.106578\n",
      "[73]\tvalid_0's binary_logloss: 0.106261\n",
      "[74]\tvalid_0's binary_logloss: 0.105959\n",
      "[75]\tvalid_0's binary_logloss: 0.10566\n",
      "[76]\tvalid_0's binary_logloss: 0.105374\n",
      "[77]\tvalid_0's binary_logloss: 0.105092\n",
      "[78]\tvalid_0's binary_logloss: 0.104822\n",
      "[79]\tvalid_0's binary_logloss: 0.104555\n",
      "[80]\tvalid_0's binary_logloss: 0.104301\n",
      "[81]\tvalid_0's binary_logloss: 0.104049\n",
      "[82]\tvalid_0's binary_logloss: 0.103808\n",
      "[83]\tvalid_0's binary_logloss: 0.10357\n",
      "[84]\tvalid_0's binary_logloss: 0.103343\n",
      "[85]\tvalid_0's binary_logloss: 0.103118\n",
      "[86]\tvalid_0's binary_logloss: 0.102903\n",
      "[87]\tvalid_0's binary_logloss: 0.102691\n",
      "[88]\tvalid_0's binary_logloss: 0.102488\n",
      "[89]\tvalid_0's binary_logloss: 0.102287\n",
      "[90]\tvalid_0's binary_logloss: 0.102095\n",
      "[91]\tvalid_0's binary_logloss: 0.101906\n",
      "[92]\tvalid_0's binary_logloss: 0.101724\n",
      "[93]\tvalid_0's binary_logloss: 0.101545\n",
      "[94]\tvalid_0's binary_logloss: 0.101373\n",
      "[95]\tvalid_0's binary_logloss: 0.101204\n",
      "[96]\tvalid_0's binary_logloss: 0.101041\n",
      "[97]\tvalid_0's binary_logloss: 0.100881\n",
      "[98]\tvalid_0's binary_logloss: 0.100727\n",
      "[99]\tvalid_0's binary_logloss: 0.100576\n",
      "[100]\tvalid_0's binary_logloss: 0.100431\n",
      "[101]\tvalid_0's binary_logloss: 0.100288\n",
      "[102]\tvalid_0's binary_logloss: 0.10015\n",
      "[103]\tvalid_0's binary_logloss: 0.100015\n",
      "[104]\tvalid_0's binary_logloss: 0.0998847\n",
      "[105]\tvalid_0's binary_logloss: 0.0997568\n",
      "[106]\tvalid_0's binary_logloss: 0.0996337\n",
      "[107]\tvalid_0's binary_logloss: 0.0995128\n",
      "[108]\tvalid_0's binary_logloss: 0.0993962\n",
      "[109]\tvalid_0's binary_logloss: 0.0992819\n",
      "[110]\tvalid_0's binary_logloss: 0.0991717\n",
      "[111]\tvalid_0's binary_logloss: 0.0990636\n",
      "[112]\tvalid_0's binary_logloss: 0.0989592\n",
      "[113]\tvalid_0's binary_logloss: 0.0988569\n",
      "[114]\tvalid_0's binary_logloss: 0.0987582\n",
      "[115]\tvalid_0's binary_logloss: 0.0986614\n",
      "[116]\tvalid_0's binary_logloss: 0.098568\n",
      "[117]\tvalid_0's binary_logloss: 0.0984766\n",
      "[118]\tvalid_0's binary_logloss: 0.098388\n",
      "[119]\tvalid_0's binary_logloss: 0.0983015\n",
      "[120]\tvalid_0's binary_logloss: 0.0982176\n",
      "[121]\tvalid_0's binary_logloss: 0.0981358\n",
      "[122]\tvalid_0's binary_logloss: 0.0980565\n",
      "[123]\tvalid_0's binary_logloss: 0.0979792\n",
      "[124]\tvalid_0's binary_logloss: 0.0979041\n",
      "[125]\tvalid_0's binary_logloss: 0.0978309\n",
      "[126]\tvalid_0's binary_logloss: 0.0977598\n",
      "[127]\tvalid_0's binary_logloss: 0.0976905\n",
      "[128]\tvalid_0's binary_logloss: 0.0976233\n",
      "[129]\tvalid_0's binary_logloss: 0.0975578\n",
      "[130]\tvalid_0's binary_logloss: 0.097494\n",
      "[131]\tvalid_0's binary_logloss: 0.097432\n",
      "[132]\tvalid_0's binary_logloss: 0.0973717\n",
      "[133]\tvalid_0's binary_logloss: 0.0973131\n",
      "[134]\tvalid_0's binary_logloss: 0.0972559\n",
      "[135]\tvalid_0's binary_logloss: 0.0972006\n",
      "[136]\tvalid_0's binary_logloss: 0.0971464\n",
      "[137]\tvalid_0's binary_logloss: 0.0970941\n",
      "[138]\tvalid_0's binary_logloss: 0.0970429\n",
      "[139]\tvalid_0's binary_logloss: 0.0969933\n",
      "[140]\tvalid_0's binary_logloss: 0.0969449\n",
      "[141]\tvalid_0's binary_logloss: 0.0968981\n",
      "[142]\tvalid_0's binary_logloss: 0.0968523\n",
      "[143]\tvalid_0's binary_logloss: 0.096808\n",
      "[144]\tvalid_0's binary_logloss: 0.0967646\n",
      "[145]\tvalid_0's binary_logloss: 0.0967227\n",
      "[146]\tvalid_0's binary_logloss: 0.0966816\n",
      "[147]\tvalid_0's binary_logloss: 0.096642\n",
      "[148]\tvalid_0's binary_logloss: 0.0966032\n",
      "[149]\tvalid_0's binary_logloss: 0.0965657\n",
      "[150]\tvalid_0's binary_logloss: 0.0965289\n",
      "[151]\tvalid_0's binary_logloss: 0.0964935\n",
      "[152]\tvalid_0's binary_logloss: 0.0964587\n",
      "[153]\tvalid_0's binary_logloss: 0.0964252\n",
      "[154]\tvalid_0's binary_logloss: 0.0963922\n",
      "[155]\tvalid_0's binary_logloss: 0.0963605\n",
      "[156]\tvalid_0's binary_logloss: 0.0963293\n",
      "[157]\tvalid_0's binary_logloss: 0.0962994\n",
      "[158]\tvalid_0's binary_logloss: 0.0962698\n",
      "[159]\tvalid_0's binary_logloss: 0.0962415\n",
      "[160]\tvalid_0's binary_logloss: 0.0962135\n",
      "[161]\tvalid_0's binary_logloss: 0.0961868\n",
      "[162]\tvalid_0's binary_logloss: 0.0961603\n",
      "[163]\tvalid_0's binary_logloss: 0.096135\n",
      "[164]\tvalid_0's binary_logloss: 0.09611\n",
      "[165]\tvalid_0's binary_logloss: 0.0960861\n",
      "[166]\tvalid_0's binary_logloss: 0.0960624\n",
      "[167]\tvalid_0's binary_logloss: 0.0960398\n",
      "[168]\tvalid_0's binary_logloss: 0.0960173\n",
      "[169]\tvalid_0's binary_logloss: 0.095996\n",
      "[170]\tvalid_0's binary_logloss: 0.0959747\n",
      "[171]\tvalid_0's binary_logloss: 0.0959545\n",
      "[172]\tvalid_0's binary_logloss: 0.0959344\n",
      "[173]\tvalid_0's binary_logloss: 0.0959153\n",
      "[174]\tvalid_0's binary_logloss: 0.0958963\n",
      "[175]\tvalid_0's binary_logloss: 0.0958782\n",
      "[176]\tvalid_0's binary_logloss: 0.0958602\n",
      "[177]\tvalid_0's binary_logloss: 0.0958432\n",
      "[178]\tvalid_0's binary_logloss: 0.0958261\n",
      "[179]\tvalid_0's binary_logloss: 0.0958099\n",
      "[180]\tvalid_0's binary_logloss: 0.0957938\n",
      "[181]\tvalid_0's binary_logloss: 0.0957785\n",
      "[182]\tvalid_0's binary_logloss: 0.0957634\n",
      "[183]\tvalid_0's binary_logloss: 0.095749\n",
      "[184]\tvalid_0's binary_logloss: 0.0957347\n",
      "[185]\tvalid_0's binary_logloss: 0.0957209\n",
      "[186]\tvalid_0's binary_logloss: 0.0957074\n",
      "[187]\tvalid_0's binary_logloss: 0.0956944\n",
      "[188]\tvalid_0's binary_logloss: 0.0956816\n",
      "[189]\tvalid_0's binary_logloss: 0.0956694\n",
      "[190]\tvalid_0's binary_logloss: 0.0956575\n",
      "[191]\tvalid_0's binary_logloss: 0.0956459\n",
      "[192]\tvalid_0's binary_logloss: 0.0956345\n",
      "[193]\tvalid_0's binary_logloss: 0.0956237\n",
      "[194]\tvalid_0's binary_logloss: 0.0956129\n",
      "[195]\tvalid_0's binary_logloss: 0.0956026\n",
      "[196]\tvalid_0's binary_logloss: 0.0955924\n",
      "[197]\tvalid_0's binary_logloss: 0.0955827\n",
      "[198]\tvalid_0's binary_logloss: 0.0955732\n",
      "[199]\tvalid_0's binary_logloss: 0.095564\n",
      "[200]\tvalid_0's binary_logloss: 0.095555\n",
      "[201]\tvalid_0's binary_logloss: 0.0955463\n",
      "[202]\tvalid_0's binary_logloss: 0.0955378\n",
      "[203]\tvalid_0's binary_logloss: 0.0955296\n",
      "[204]\tvalid_0's binary_logloss: 0.0955216\n",
      "[205]\tvalid_0's binary_logloss: 0.0955138\n",
      "[206]\tvalid_0's binary_logloss: 0.0955063\n",
      "[207]\tvalid_0's binary_logloss: 0.095499\n",
      "[208]\tvalid_0's binary_logloss: 0.0954919\n",
      "[209]\tvalid_0's binary_logloss: 0.0954851\n",
      "[210]\tvalid_0's binary_logloss: 0.0954785\n",
      "[211]\tvalid_0's binary_logloss: 0.095472\n",
      "[212]\tvalid_0's binary_logloss: 0.0954658\n",
      "[213]\tvalid_0's binary_logloss: 0.0954597\n",
      "[214]\tvalid_0's binary_logloss: 0.0954538\n",
      "[215]\tvalid_0's binary_logloss: 0.0954481\n",
      "[216]\tvalid_0's binary_logloss: 0.0954425\n",
      "[217]\tvalid_0's binary_logloss: 0.095437\n",
      "[218]\tvalid_0's binary_logloss: 0.0954318\n",
      "[219]\tvalid_0's binary_logloss: 0.0954267\n",
      "[220]\tvalid_0's binary_logloss: 0.0954218\n",
      "[221]\tvalid_0's binary_logloss: 0.095417\n",
      "[222]\tvalid_0's binary_logloss: 0.0954124\n",
      "[223]\tvalid_0's binary_logloss: 0.0954078\n",
      "[224]\tvalid_0's binary_logloss: 0.0954034\n",
      "[225]\tvalid_0's binary_logloss: 0.0953991\n",
      "[226]\tvalid_0's binary_logloss: 0.095395\n",
      "[227]\tvalid_0's binary_logloss: 0.0953909\n",
      "[228]\tvalid_0's binary_logloss: 0.095387\n",
      "[229]\tvalid_0's binary_logloss: 0.095383\n",
      "[230]\tvalid_0's binary_logloss: 0.0953794\n",
      "[231]\tvalid_0's binary_logloss: 0.0953757\n",
      "[232]\tvalid_0's binary_logloss: 0.0953723\n",
      "[233]\tvalid_0's binary_logloss: 0.0953689\n",
      "[234]\tvalid_0's binary_logloss: 0.0953657\n",
      "[235]\tvalid_0's binary_logloss: 0.0953624\n",
      "[236]\tvalid_0's binary_logloss: 0.0953595\n",
      "[237]\tvalid_0's binary_logloss: 0.0953563\n",
      "[238]\tvalid_0's binary_logloss: 0.0953535\n",
      "[239]\tvalid_0's binary_logloss: 0.0953505\n",
      "[240]\tvalid_0's binary_logloss: 0.0953478\n",
      "[241]\tvalid_0's binary_logloss: 0.095345\n",
      "[242]\tvalid_0's binary_logloss: 0.0953425\n",
      "[243]\tvalid_0's binary_logloss: 0.0953398\n",
      "[244]\tvalid_0's binary_logloss: 0.0953375\n",
      "[245]\tvalid_0's binary_logloss: 0.095335\n",
      "[246]\tvalid_0's binary_logloss: 0.095333\n",
      "[247]\tvalid_0's binary_logloss: 0.0953306\n",
      "[248]\tvalid_0's binary_logloss: 0.0953287\n",
      "[249]\tvalid_0's binary_logloss: 0.0953265\n",
      "[250]\tvalid_0's binary_logloss: 0.0953247\n",
      "[251]\tvalid_0's binary_logloss: 0.0953225\n",
      "[252]\tvalid_0's binary_logloss: 0.0953208\n",
      "[253]\tvalid_0's binary_logloss: 0.0953189\n",
      "[254]\tvalid_0's binary_logloss: 0.0953175\n",
      "[255]\tvalid_0's binary_logloss: 0.0953156\n",
      "[256]\tvalid_0's binary_logloss: 0.0953142\n",
      "[257]\tvalid_0's binary_logloss: 0.0953125\n",
      "[258]\tvalid_0's binary_logloss: 0.0953112\n",
      "[259]\tvalid_0's binary_logloss: 0.0953096\n",
      "[260]\tvalid_0's binary_logloss: 0.0953084\n",
      "[261]\tvalid_0's binary_logloss: 0.0953068\n",
      "[262]\tvalid_0's binary_logloss: 0.0953057\n",
      "[263]\tvalid_0's binary_logloss: 0.095304\n",
      "[264]\tvalid_0's binary_logloss: 0.0953029\n",
      "[265]\tvalid_0's binary_logloss: 0.0953015\n",
      "[266]\tvalid_0's binary_logloss: 0.0953005\n",
      "[267]\tvalid_0's binary_logloss: 0.095299\n",
      "[268]\tvalid_0's binary_logloss: 0.0952982\n",
      "[269]\tvalid_0's binary_logloss: 0.0952968\n",
      "[270]\tvalid_0's binary_logloss: 0.095296\n",
      "[271]\tvalid_0's binary_logloss: 0.0952947\n",
      "[272]\tvalid_0's binary_logloss: 0.095294\n",
      "[273]\tvalid_0's binary_logloss: 0.0952928\n",
      "[274]\tvalid_0's binary_logloss: 0.0952921\n",
      "[275]\tvalid_0's binary_logloss: 0.095291\n",
      "[276]\tvalid_0's binary_logloss: 0.0952904\n",
      "[277]\tvalid_0's binary_logloss: 0.0952893\n",
      "[278]\tvalid_0's binary_logloss: 0.0952887\n",
      "[279]\tvalid_0's binary_logloss: 0.0952877\n",
      "[280]\tvalid_0's binary_logloss: 0.0952873\n",
      "[281]\tvalid_0's binary_logloss: 0.0952863\n",
      "[282]\tvalid_0's binary_logloss: 0.095286\n",
      "[283]\tvalid_0's binary_logloss: 0.095285\n",
      "[284]\tvalid_0's binary_logloss: 0.0952845\n",
      "[285]\tvalid_0's binary_logloss: 0.0952836\n",
      "[286]\tvalid_0's binary_logloss: 0.0952833\n",
      "[287]\tvalid_0's binary_logloss: 0.0952825\n",
      "[288]\tvalid_0's binary_logloss: 0.0952822\n",
      "[289]\tvalid_0's binary_logloss: 0.0952813\n",
      "[290]\tvalid_0's binary_logloss: 0.0952811\n",
      "[291]\tvalid_0's binary_logloss: 0.0952803\n",
      "[292]\tvalid_0's binary_logloss: 0.0952801\n",
      "[293]\tvalid_0's binary_logloss: 0.0952794\n",
      "[294]\tvalid_0's binary_logloss: 0.0952793\n",
      "[295]\tvalid_0's binary_logloss: 0.0952785\n",
      "[296]\tvalid_0's binary_logloss: 0.0952784\n",
      "[297]\tvalid_0's binary_logloss: 0.0952777\n",
      "[298]\tvalid_0's binary_logloss: 0.0952777\n",
      "[299]\tvalid_0's binary_logloss: 0.0952769\n",
      "[300]\tvalid_0's binary_logloss: 0.0952769\n",
      "[301]\tvalid_0's binary_logloss: 0.0952763\n",
      "[302]\tvalid_0's binary_logloss: 0.0952763\n",
      "[303]\tvalid_0's binary_logloss: 0.0952757\n",
      "[304]\tvalid_0's binary_logloss: 0.0952758\n",
      "[305]\tvalid_0's binary_logloss: 0.0952751\n",
      "[306]\tvalid_0's binary_logloss: 0.0952752\n",
      "[307]\tvalid_0's binary_logloss: 0.0952746\n",
      "[308]\tvalid_0's binary_logloss: 0.0952747\n",
      "[309]\tvalid_0's binary_logloss: 0.095274\n",
      "[310]\tvalid_0's binary_logloss: 0.0952742\n",
      "[311]\tvalid_0's binary_logloss: 0.0952736\n",
      "[312]\tvalid_0's binary_logloss: 0.0952738\n",
      "[313]\tvalid_0's binary_logloss: 0.0952732\n",
      "[314]\tvalid_0's binary_logloss: 0.0952734\n",
      "[315]\tvalid_0's binary_logloss: 0.0952729\n",
      "[316]\tvalid_0's binary_logloss: 0.0952732\n",
      "[317]\tvalid_0's binary_logloss: 0.0952726\n",
      "[318]\tvalid_0's binary_logloss: 0.0952729\n",
      "[319]\tvalid_0's binary_logloss: 0.0952724\n",
      "[320]\tvalid_0's binary_logloss: 0.0952726\n",
      "[321]\tvalid_0's binary_logloss: 0.0952721\n",
      "[322]\tvalid_0's binary_logloss: 0.0952724\n",
      "[323]\tvalid_0's binary_logloss: 0.0952719\n",
      "[324]\tvalid_0's binary_logloss: 0.0952722\n",
      "[325]\tvalid_0's binary_logloss: 0.0952718\n",
      "[326]\tvalid_0's binary_logloss: 0.0952721\n",
      "[327]\tvalid_0's binary_logloss: 0.0952717\n",
      "[328]\tvalid_0's binary_logloss: 0.095272\n",
      "[329]\tvalid_0's binary_logloss: 0.0952717\n",
      "[330]\tvalid_0's binary_logloss: 0.0952721\n",
      "[331]\tvalid_0's binary_logloss: 0.0952717\n",
      "[332]\tvalid_0's binary_logloss: 0.095272\n",
      "[333]\tvalid_0's binary_logloss: 0.0952717\n",
      "[334]\tvalid_0's binary_logloss: 0.0952721\n",
      "[335]\tvalid_0's binary_logloss: 0.0952717\n",
      "[336]\tvalid_0's binary_logloss: 0.0952722\n",
      "[337]\tvalid_0's binary_logloss: 0.0952719\n",
      "[338]\tvalid_0's binary_logloss: 0.0952723\n",
      "[339]\tvalid_0's binary_logloss: 0.0952719\n",
      "[340]\tvalid_0's binary_logloss: 0.0952724\n",
      "[341]\tvalid_0's binary_logloss: 0.095272\n",
      "[342]\tvalid_0's binary_logloss: 0.0952725\n",
      "[343]\tvalid_0's binary_logloss: 0.0952723\n",
      "[344]\tvalid_0's binary_logloss: 0.0952728\n",
      "[345]\tvalid_0's binary_logloss: 0.0952725\n",
      "[346]\tvalid_0's binary_logloss: 0.0952729\n",
      "[347]\tvalid_0's binary_logloss: 0.0952726\n",
      "[348]\tvalid_0's binary_logloss: 0.095273\n",
      "[349]\tvalid_0's binary_logloss: 0.0952728\n",
      "[350]\tvalid_0's binary_logloss: 0.0952732\n",
      "[351]\tvalid_0's binary_logloss: 0.095273\n",
      "[352]\tvalid_0's binary_logloss: 0.0952735\n",
      "[353]\tvalid_0's binary_logloss: 0.0952733\n",
      "Early stopping, best iteration is:\n",
      "[333]\tvalid_0's binary_logloss: 0.0952717\n",
      "2nd level Fold 2\n",
      "[1]\tvalid_0's binary_logloss: 0.221719\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.213497\n",
      "[3]\tvalid_0's binary_logloss: 0.20642\n",
      "[4]\tvalid_0's binary_logloss: 0.20037\n",
      "[5]\tvalid_0's binary_logloss: 0.194936\n",
      "[6]\tvalid_0's binary_logloss: 0.190124\n",
      "[7]\tvalid_0's binary_logloss: 0.185703\n",
      "[8]\tvalid_0's binary_logloss: 0.181706\n",
      "[9]\tvalid_0's binary_logloss: 0.177982\n",
      "[10]\tvalid_0's binary_logloss: 0.174569\n",
      "[11]\tvalid_0's binary_logloss: 0.171359\n",
      "[12]\tvalid_0's binary_logloss: 0.168388\n",
      "[13]\tvalid_0's binary_logloss: 0.165574\n",
      "[14]\tvalid_0's binary_logloss: 0.162951\n",
      "[15]\tvalid_0's binary_logloss: 0.160453\n",
      "[16]\tvalid_0's binary_logloss: 0.158112\n",
      "[17]\tvalid_0's binary_logloss: 0.155874\n",
      "[18]\tvalid_0's binary_logloss: 0.153766\n",
      "[19]\tvalid_0's binary_logloss: 0.151746\n",
      "[20]\tvalid_0's binary_logloss: 0.149835\n",
      "[21]\tvalid_0's binary_logloss: 0.147999\n",
      "[22]\tvalid_0's binary_logloss: 0.146258\n",
      "[23]\tvalid_0's binary_logloss: 0.144581\n",
      "[24]\tvalid_0's binary_logloss: 0.142987\n",
      "[25]\tvalid_0's binary_logloss: 0.141449\n",
      "[26]\tvalid_0's binary_logloss: 0.139983\n",
      "[27]\tvalid_0's binary_logloss: 0.138567\n",
      "[28]\tvalid_0's binary_logloss: 0.137216\n",
      "[29]\tvalid_0's binary_logloss: 0.135909\n",
      "[30]\tvalid_0's binary_logloss: 0.134658\n",
      "[31]\tvalid_0's binary_logloss: 0.133447\n",
      "[32]\tvalid_0's binary_logloss: 0.132288\n",
      "[33]\tvalid_0's binary_logloss: 0.131164\n",
      "[34]\tvalid_0's binary_logloss: 0.130087\n",
      "[35]\tvalid_0's binary_logloss: 0.129042\n",
      "[36]\tvalid_0's binary_logloss: 0.128038\n",
      "[37]\tvalid_0's binary_logloss: 0.127064\n",
      "[38]\tvalid_0's binary_logloss: 0.126129\n",
      "[39]\tvalid_0's binary_logloss: 0.12522\n",
      "[40]\tvalid_0's binary_logloss: 0.124346\n",
      "[41]\tvalid_0's binary_logloss: 0.123497\n",
      "[42]\tvalid_0's binary_logloss: 0.122679\n",
      "[43]\tvalid_0's binary_logloss: 0.121885\n",
      "[44]\tvalid_0's binary_logloss: 0.121119\n",
      "[45]\tvalid_0's binary_logloss: 0.120375\n",
      "[46]\tvalid_0's binary_logloss: 0.119658\n",
      "[47]\tvalid_0's binary_logloss: 0.11896\n",
      "[48]\tvalid_0's binary_logloss: 0.118286\n",
      "[49]\tvalid_0's binary_logloss: 0.117631\n",
      "[50]\tvalid_0's binary_logloss: 0.116999\n",
      "[51]\tvalid_0's binary_logloss: 0.116384\n",
      "[52]\tvalid_0's binary_logloss: 0.11579\n",
      "[53]\tvalid_0's binary_logloss: 0.115211\n",
      "[54]\tvalid_0's binary_logloss: 0.114652\n",
      "[55]\tvalid_0's binary_logloss: 0.114108\n",
      "[56]\tvalid_0's binary_logloss: 0.113582\n",
      "[57]\tvalid_0's binary_logloss: 0.11307\n",
      "[58]\tvalid_0's binary_logloss: 0.112574\n",
      "[59]\tvalid_0's binary_logloss: 0.112092\n",
      "[60]\tvalid_0's binary_logloss: 0.111625\n",
      "[61]\tvalid_0's binary_logloss: 0.11117\n",
      "[62]\tvalid_0's binary_logloss: 0.11073\n",
      "[63]\tvalid_0's binary_logloss: 0.110302\n",
      "[64]\tvalid_0's binary_logloss: 0.109886\n",
      "[65]\tvalid_0's binary_logloss: 0.109482\n",
      "[66]\tvalid_0's binary_logloss: 0.109091\n",
      "[67]\tvalid_0's binary_logloss: 0.108709\n",
      "[68]\tvalid_0's binary_logloss: 0.108339\n",
      "[69]\tvalid_0's binary_logloss: 0.10798\n",
      "[70]\tvalid_0's binary_logloss: 0.10763\n",
      "[71]\tvalid_0's binary_logloss: 0.10729\n",
      "[72]\tvalid_0's binary_logloss: 0.10696\n",
      "[73]\tvalid_0's binary_logloss: 0.106639\n",
      "[74]\tvalid_0's binary_logloss: 0.106327\n",
      "[75]\tvalid_0's binary_logloss: 0.106024\n",
      "[76]\tvalid_0's binary_logloss: 0.105729\n",
      "[77]\tvalid_0's binary_logloss: 0.105443\n",
      "[78]\tvalid_0's binary_logloss: 0.105164\n",
      "[79]\tvalid_0's binary_logloss: 0.104893\n",
      "[80]\tvalid_0's binary_logloss: 0.10463\n",
      "[81]\tvalid_0's binary_logloss: 0.104374\n",
      "[82]\tvalid_0's binary_logloss: 0.104124\n",
      "[83]\tvalid_0's binary_logloss: 0.103882\n",
      "[84]\tvalid_0's binary_logloss: 0.103646\n",
      "[85]\tvalid_0's binary_logloss: 0.103417\n",
      "[86]\tvalid_0's binary_logloss: 0.103194\n",
      "[87]\tvalid_0's binary_logloss: 0.102977\n",
      "[88]\tvalid_0's binary_logloss: 0.102766\n",
      "[89]\tvalid_0's binary_logloss: 0.102561\n",
      "[90]\tvalid_0's binary_logloss: 0.10236\n",
      "[91]\tvalid_0's binary_logloss: 0.102166\n",
      "[92]\tvalid_0's binary_logloss: 0.101977\n",
      "[93]\tvalid_0's binary_logloss: 0.101793\n",
      "[94]\tvalid_0's binary_logloss: 0.101614\n",
      "[95]\tvalid_0's binary_logloss: 0.10144\n",
      "[96]\tvalid_0's binary_logloss: 0.10127\n",
      "[97]\tvalid_0's binary_logloss: 0.101105\n",
      "[98]\tvalid_0's binary_logloss: 0.100944\n",
      "[99]\tvalid_0's binary_logloss: 0.100788\n",
      "[100]\tvalid_0's binary_logloss: 0.100635\n",
      "[101]\tvalid_0's binary_logloss: 0.100488\n",
      "[102]\tvalid_0's binary_logloss: 0.100343\n",
      "[103]\tvalid_0's binary_logloss: 0.100203\n",
      "[104]\tvalid_0's binary_logloss: 0.100066\n",
      "[105]\tvalid_0's binary_logloss: 0.0999339\n",
      "[106]\tvalid_0's binary_logloss: 0.0998038\n",
      "[107]\tvalid_0's binary_logloss: 0.0996784\n",
      "[108]\tvalid_0's binary_logloss: 0.0995551\n",
      "[109]\tvalid_0's binary_logloss: 0.0994364\n",
      "[110]\tvalid_0's binary_logloss: 0.0993195\n",
      "[111]\tvalid_0's binary_logloss: 0.099207\n",
      "[112]\tvalid_0's binary_logloss: 0.0990962\n",
      "[113]\tvalid_0's binary_logloss: 0.0989896\n",
      "[114]\tvalid_0's binary_logloss: 0.0988845\n",
      "[115]\tvalid_0's binary_logloss: 0.0987835\n",
      "[116]\tvalid_0's binary_logloss: 0.0986838\n",
      "[117]\tvalid_0's binary_logloss: 0.0985881\n",
      "[118]\tvalid_0's binary_logloss: 0.0984937\n",
      "[119]\tvalid_0's binary_logloss: 0.098403\n",
      "[120]\tvalid_0's binary_logloss: 0.0983134\n",
      "[121]\tvalid_0's binary_logloss: 0.0982275\n",
      "[122]\tvalid_0's binary_logloss: 0.0981424\n",
      "[123]\tvalid_0's binary_logloss: 0.098061\n",
      "[124]\tvalid_0's binary_logloss: 0.0979804\n",
      "[125]\tvalid_0's binary_logloss: 0.0979031\n",
      "[126]\tvalid_0's binary_logloss: 0.0978266\n",
      "[127]\tvalid_0's binary_logloss: 0.0977534\n",
      "[128]\tvalid_0's binary_logloss: 0.0976809\n",
      "[129]\tvalid_0's binary_logloss: 0.0976115\n",
      "[130]\tvalid_0's binary_logloss: 0.0975426\n",
      "[131]\tvalid_0's binary_logloss: 0.0974768\n",
      "[132]\tvalid_0's binary_logloss: 0.0974114\n",
      "[133]\tvalid_0's binary_logloss: 0.097349\n",
      "[134]\tvalid_0's binary_logloss: 0.097287\n",
      "[135]\tvalid_0's binary_logloss: 0.0972279\n",
      "[136]\tvalid_0's binary_logloss: 0.097169\n",
      "[137]\tvalid_0's binary_logloss: 0.097113\n",
      "[138]\tvalid_0's binary_logloss: 0.0970571\n",
      "[139]\tvalid_0's binary_logloss: 0.097004\n",
      "[140]\tvalid_0's binary_logloss: 0.096951\n",
      "[141]\tvalid_0's binary_logloss: 0.0969006\n",
      "[142]\tvalid_0's binary_logloss: 0.0968503\n",
      "[143]\tvalid_0's binary_logloss: 0.0968025\n",
      "[144]\tvalid_0's binary_logloss: 0.0967547\n",
      "[145]\tvalid_0's binary_logloss: 0.0967094\n",
      "[146]\tvalid_0's binary_logloss: 0.0966641\n",
      "[147]\tvalid_0's binary_logloss: 0.0966211\n",
      "[148]\tvalid_0's binary_logloss: 0.0965781\n",
      "[149]\tvalid_0's binary_logloss: 0.0965374\n",
      "[150]\tvalid_0's binary_logloss: 0.0964966\n",
      "[151]\tvalid_0's binary_logloss: 0.096458\n",
      "[152]\tvalid_0's binary_logloss: 0.0964192\n",
      "[153]\tvalid_0's binary_logloss: 0.0963826\n",
      "[154]\tvalid_0's binary_logloss: 0.0963459\n",
      "[155]\tvalid_0's binary_logloss: 0.0963112\n",
      "[156]\tvalid_0's binary_logloss: 0.0962763\n",
      "[157]\tvalid_0's binary_logloss: 0.0962434\n",
      "[158]\tvalid_0's binary_logloss: 0.0962102\n",
      "[159]\tvalid_0's binary_logloss: 0.096179\n",
      "[160]\tvalid_0's binary_logloss: 0.0961475\n",
      "[161]\tvalid_0's binary_logloss: 0.0961179\n",
      "[162]\tvalid_0's binary_logloss: 0.0960881\n",
      "[163]\tvalid_0's binary_logloss: 0.0960601\n",
      "[164]\tvalid_0's binary_logloss: 0.0960317\n",
      "[165]\tvalid_0's binary_logloss: 0.0960052\n",
      "[166]\tvalid_0's binary_logloss: 0.0959782\n",
      "[167]\tvalid_0's binary_logloss: 0.0959531\n",
      "[168]\tvalid_0's binary_logloss: 0.0959274\n",
      "[169]\tvalid_0's binary_logloss: 0.0959035\n",
      "[170]\tvalid_0's binary_logloss: 0.0958792\n",
      "[171]\tvalid_0's binary_logloss: 0.0958566\n",
      "[172]\tvalid_0's binary_logloss: 0.0958335\n",
      "[173]\tvalid_0's binary_logloss: 0.095812\n",
      "[174]\tvalid_0's binary_logloss: 0.0957901\n",
      "[175]\tvalid_0's binary_logloss: 0.0957698\n",
      "[176]\tvalid_0's binary_logloss: 0.095749\n",
      "[177]\tvalid_0's binary_logloss: 0.0957297\n",
      "[178]\tvalid_0's binary_logloss: 0.09571\n",
      "[179]\tvalid_0's binary_logloss: 0.0956918\n",
      "[180]\tvalid_0's binary_logloss: 0.095673\n",
      "[181]\tvalid_0's binary_logloss: 0.0956558\n",
      "[182]\tvalid_0's binary_logloss: 0.095638\n",
      "[183]\tvalid_0's binary_logloss: 0.0956216\n",
      "[184]\tvalid_0's binary_logloss: 0.0956048\n",
      "[185]\tvalid_0's binary_logloss: 0.0955893\n",
      "[186]\tvalid_0's binary_logloss: 0.0955733\n",
      "[187]\tvalid_0's binary_logloss: 0.0955586\n",
      "[188]\tvalid_0's binary_logloss: 0.0955434\n",
      "[189]\tvalid_0's binary_logloss: 0.0955295\n",
      "[190]\tvalid_0's binary_logloss: 0.0955151\n",
      "[191]\tvalid_0's binary_logloss: 0.0955019\n",
      "[192]\tvalid_0's binary_logloss: 0.0954882\n",
      "[193]\tvalid_0's binary_logloss: 0.0954757\n",
      "[194]\tvalid_0's binary_logloss: 0.0954626\n",
      "[195]\tvalid_0's binary_logloss: 0.0954508\n",
      "[196]\tvalid_0's binary_logloss: 0.0954384\n",
      "[197]\tvalid_0's binary_logloss: 0.0954271\n",
      "[198]\tvalid_0's binary_logloss: 0.0954154\n",
      "[199]\tvalid_0's binary_logloss: 0.0954046\n",
      "[200]\tvalid_0's binary_logloss: 0.0953935\n",
      "[201]\tvalid_0's binary_logloss: 0.0953834\n",
      "[202]\tvalid_0's binary_logloss: 0.0953728\n",
      "[203]\tvalid_0's binary_logloss: 0.0953632\n",
      "[204]\tvalid_0's binary_logloss: 0.0953531\n",
      "[205]\tvalid_0's binary_logloss: 0.0953441\n",
      "[206]\tvalid_0's binary_logloss: 0.0953346\n",
      "[207]\tvalid_0's binary_logloss: 0.095326\n",
      "[208]\tvalid_0's binary_logloss: 0.0953169\n",
      "[209]\tvalid_0's binary_logloss: 0.0953089\n",
      "[210]\tvalid_0's binary_logloss: 0.0953002\n",
      "[211]\tvalid_0's binary_logloss: 0.0952924\n",
      "[212]\tvalid_0's binary_logloss: 0.0952841\n",
      "[213]\tvalid_0's binary_logloss: 0.0952769\n",
      "[214]\tvalid_0's binary_logloss: 0.0952691\n",
      "[215]\tvalid_0's binary_logloss: 0.0952622\n",
      "[216]\tvalid_0's binary_logloss: 0.0952549\n",
      "[217]\tvalid_0's binary_logloss: 0.0952484\n",
      "[218]\tvalid_0's binary_logloss: 0.0952414\n",
      "[219]\tvalid_0's binary_logloss: 0.0952353\n",
      "[220]\tvalid_0's binary_logloss: 0.0952287\n",
      "[221]\tvalid_0's binary_logloss: 0.095223\n",
      "[222]\tvalid_0's binary_logloss: 0.0952166\n",
      "[223]\tvalid_0's binary_logloss: 0.0952111\n",
      "[224]\tvalid_0's binary_logloss: 0.095205\n",
      "[225]\tvalid_0's binary_logloss: 0.0951998\n",
      "[226]\tvalid_0's binary_logloss: 0.0951941\n",
      "[227]\tvalid_0's binary_logloss: 0.0951893\n",
      "[228]\tvalid_0's binary_logloss: 0.095184\n",
      "[229]\tvalid_0's binary_logloss: 0.0951794\n",
      "[230]\tvalid_0's binary_logloss: 0.0951744\n",
      "[231]\tvalid_0's binary_logloss: 0.0951701\n",
      "[232]\tvalid_0's binary_logloss: 0.0951654\n",
      "[233]\tvalid_0's binary_logloss: 0.0951613\n",
      "[234]\tvalid_0's binary_logloss: 0.0951568\n",
      "[235]\tvalid_0's binary_logloss: 0.095153\n",
      "[236]\tvalid_0's binary_logloss: 0.0951487\n",
      "[237]\tvalid_0's binary_logloss: 0.0951451\n",
      "[238]\tvalid_0's binary_logloss: 0.0951411\n",
      "[239]\tvalid_0's binary_logloss: 0.0951376\n",
      "[240]\tvalid_0's binary_logloss: 0.0951338\n",
      "[241]\tvalid_0's binary_logloss: 0.0951306\n",
      "[242]\tvalid_0's binary_logloss: 0.095127\n",
      "[243]\tvalid_0's binary_logloss: 0.095124\n",
      "[244]\tvalid_0's binary_logloss: 0.0951206\n",
      "[245]\tvalid_0's binary_logloss: 0.0951177\n",
      "[246]\tvalid_0's binary_logloss: 0.0951146\n",
      "[247]\tvalid_0's binary_logloss: 0.0951118\n",
      "[248]\tvalid_0's binary_logloss: 0.0951088\n",
      "[249]\tvalid_0's binary_logloss: 0.0951062\n",
      "[250]\tvalid_0's binary_logloss: 0.0951033\n",
      "[251]\tvalid_0's binary_logloss: 0.0951009\n",
      "[252]\tvalid_0's binary_logloss: 0.0950983\n",
      "[253]\tvalid_0's binary_logloss: 0.095096\n",
      "[254]\tvalid_0's binary_logloss: 0.0950934\n",
      "[255]\tvalid_0's binary_logloss: 0.0950913\n",
      "[256]\tvalid_0's binary_logloss: 0.0950888\n",
      "[257]\tvalid_0's binary_logloss: 0.0950868\n",
      "[258]\tvalid_0's binary_logloss: 0.0950845\n",
      "[259]\tvalid_0's binary_logloss: 0.0950827\n",
      "[260]\tvalid_0's binary_logloss: 0.0950805\n",
      "[261]\tvalid_0's binary_logloss: 0.0950789\n",
      "[262]\tvalid_0's binary_logloss: 0.0950768\n",
      "[263]\tvalid_0's binary_logloss: 0.0950752\n",
      "[264]\tvalid_0's binary_logloss: 0.0950733\n",
      "[265]\tvalid_0's binary_logloss: 0.0950718\n",
      "[266]\tvalid_0's binary_logloss: 0.0950701\n",
      "[267]\tvalid_0's binary_logloss: 0.0950686\n",
      "[268]\tvalid_0's binary_logloss: 0.095067\n",
      "[269]\tvalid_0's binary_logloss: 0.0950658\n",
      "[270]\tvalid_0's binary_logloss: 0.0950642\n",
      "[271]\tvalid_0's binary_logloss: 0.095063\n",
      "[272]\tvalid_0's binary_logloss: 0.0950615\n",
      "[273]\tvalid_0's binary_logloss: 0.0950604\n",
      "[274]\tvalid_0's binary_logloss: 0.095059\n",
      "[275]\tvalid_0's binary_logloss: 0.095058\n",
      "[276]\tvalid_0's binary_logloss: 0.0950567\n",
      "[277]\tvalid_0's binary_logloss: 0.0950557\n",
      "[278]\tvalid_0's binary_logloss: 0.0950545\n",
      "[279]\tvalid_0's binary_logloss: 0.0950535\n",
      "[280]\tvalid_0's binary_logloss: 0.0950523\n",
      "[281]\tvalid_0's binary_logloss: 0.0950515\n",
      "[282]\tvalid_0's binary_logloss: 0.0950506\n",
      "[283]\tvalid_0's binary_logloss: 0.0950499\n",
      "[284]\tvalid_0's binary_logloss: 0.0950489\n",
      "[285]\tvalid_0's binary_logloss: 0.0950483\n",
      "[286]\tvalid_0's binary_logloss: 0.0950473\n",
      "[287]\tvalid_0's binary_logloss: 0.0950467\n",
      "[288]\tvalid_0's binary_logloss: 0.0950458\n",
      "[289]\tvalid_0's binary_logloss: 0.0950453\n",
      "[290]\tvalid_0's binary_logloss: 0.0950444\n",
      "[291]\tvalid_0's binary_logloss: 0.095044\n",
      "[292]\tvalid_0's binary_logloss: 0.0950434\n",
      "[293]\tvalid_0's binary_logloss: 0.0950429\n",
      "[294]\tvalid_0's binary_logloss: 0.0950423\n",
      "[295]\tvalid_0's binary_logloss: 0.0950419\n",
      "[296]\tvalid_0's binary_logloss: 0.0950413\n",
      "[297]\tvalid_0's binary_logloss: 0.0950409\n",
      "[298]\tvalid_0's binary_logloss: 0.0950404\n",
      "[299]\tvalid_0's binary_logloss: 0.09504\n",
      "[300]\tvalid_0's binary_logloss: 0.0950395\n",
      "[301]\tvalid_0's binary_logloss: 0.095039\n",
      "[302]\tvalid_0's binary_logloss: 0.0950386\n",
      "[303]\tvalid_0's binary_logloss: 0.0950381\n",
      "[304]\tvalid_0's binary_logloss: 0.0950377\n",
      "[305]\tvalid_0's binary_logloss: 0.0950374\n",
      "[306]\tvalid_0's binary_logloss: 0.095037\n",
      "[307]\tvalid_0's binary_logloss: 0.0950368\n",
      "[308]\tvalid_0's binary_logloss: 0.0950363\n",
      "[309]\tvalid_0's binary_logloss: 0.095036\n",
      "[310]\tvalid_0's binary_logloss: 0.0950356\n",
      "[311]\tvalid_0's binary_logloss: 0.0950354\n",
      "[312]\tvalid_0's binary_logloss: 0.095035\n",
      "[313]\tvalid_0's binary_logloss: 0.0950349\n",
      "[314]\tvalid_0's binary_logloss: 0.0950347\n",
      "[315]\tvalid_0's binary_logloss: 0.0950346\n",
      "[316]\tvalid_0's binary_logloss: 0.0950344\n",
      "[317]\tvalid_0's binary_logloss: 0.0950343\n",
      "[318]\tvalid_0's binary_logloss: 0.0950341\n",
      "[319]\tvalid_0's binary_logloss: 0.0950341\n",
      "[320]\tvalid_0's binary_logloss: 0.0950339\n",
      "[321]\tvalid_0's binary_logloss: 0.0950339\n",
      "[322]\tvalid_0's binary_logloss: 0.0950338\n",
      "[323]\tvalid_0's binary_logloss: 0.0950337\n",
      "[324]\tvalid_0's binary_logloss: 0.0950336\n",
      "[325]\tvalid_0's binary_logloss: 0.0950337\n",
      "[326]\tvalid_0's binary_logloss: 0.0950335\n",
      "[327]\tvalid_0's binary_logloss: 0.0950335\n",
      "[328]\tvalid_0's binary_logloss: 0.0950335\n",
      "[329]\tvalid_0's binary_logloss: 0.0950335\n",
      "[330]\tvalid_0's binary_logloss: 0.0950335\n",
      "[331]\tvalid_0's binary_logloss: 0.0950335\n",
      "[332]\tvalid_0's binary_logloss: 0.0950335\n",
      "[333]\tvalid_0's binary_logloss: 0.0950336\n",
      "[334]\tvalid_0's binary_logloss: 0.0950335\n",
      "[335]\tvalid_0's binary_logloss: 0.0950337\n",
      "[336]\tvalid_0's binary_logloss: 0.0950338\n",
      "[337]\tvalid_0's binary_logloss: 0.095034\n",
      "[338]\tvalid_0's binary_logloss: 0.095034\n",
      "[339]\tvalid_0's binary_logloss: 0.0950342\n",
      "[340]\tvalid_0's binary_logloss: 0.0950342\n",
      "[341]\tvalid_0's binary_logloss: 0.0950344\n",
      "[342]\tvalid_0's binary_logloss: 0.0950344\n",
      "[343]\tvalid_0's binary_logloss: 0.0950347\n",
      "[344]\tvalid_0's binary_logloss: 0.0950347\n",
      "[345]\tvalid_0's binary_logloss: 0.0950349\n",
      "[346]\tvalid_0's binary_logloss: 0.0950349\n",
      "[347]\tvalid_0's binary_logloss: 0.0950352\n",
      "[348]\tvalid_0's binary_logloss: 0.0950352\n",
      "[349]\tvalid_0's binary_logloss: 0.0950355\n",
      "[350]\tvalid_0's binary_logloss: 0.0950356\n",
      "Early stopping, best iteration is:\n",
      "[330]\tvalid_0's binary_logloss: 0.0950335\n",
      "2nd level Fold 3\n",
      "[1]\tvalid_0's binary_logloss: 0.22236\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.214191\n",
      "[3]\tvalid_0's binary_logloss: 0.207438\n",
      "[4]\tvalid_0's binary_logloss: 0.201412\n",
      "[5]\tvalid_0's binary_logloss: 0.196179\n",
      "[6]\tvalid_0's binary_logloss: 0.191377\n",
      "[7]\tvalid_0's binary_logloss: 0.187095\n",
      "[8]\tvalid_0's binary_logloss: 0.183099\n",
      "[9]\tvalid_0's binary_logloss: 0.179476\n",
      "[10]\tvalid_0's binary_logloss: 0.176059\n",
      "[11]\tvalid_0's binary_logloss: 0.172925\n",
      "[12]\tvalid_0's binary_logloss: 0.169946\n",
      "[13]\tvalid_0's binary_logloss: 0.167191\n",
      "[14]\tvalid_0's binary_logloss: 0.164557\n",
      "[15]\tvalid_0's binary_logloss: 0.162106\n",
      "[16]\tvalid_0's binary_logloss: 0.159753\n",
      "[17]\tvalid_0's binary_logloss: 0.157551\n",
      "[18]\tvalid_0's binary_logloss: 0.15543\n",
      "[19]\tvalid_0's binary_logloss: 0.153439\n",
      "[20]\tvalid_0's binary_logloss: 0.151515\n",
      "[21]\tvalid_0's binary_logloss: 0.149703\n",
      "[22]\tvalid_0's binary_logloss: 0.147948\n",
      "[23]\tvalid_0's binary_logloss: 0.14629\n",
      "[24]\tvalid_0's binary_logloss: 0.144681\n",
      "[25]\tvalid_0's binary_logloss: 0.143158\n",
      "[26]\tvalid_0's binary_logloss: 0.141677\n",
      "[27]\tvalid_0's binary_logloss: 0.140273\n",
      "[28]\tvalid_0's binary_logloss: 0.138906\n",
      "[29]\tvalid_0's binary_logloss: 0.137609\n",
      "[30]\tvalid_0's binary_logloss: 0.136343\n",
      "[31]\tvalid_0's binary_logloss: 0.13514\n",
      "[32]\tvalid_0's binary_logloss: 0.133966\n",
      "[33]\tvalid_0's binary_logloss: 0.132848\n",
      "[34]\tvalid_0's binary_logloss: 0.131756\n",
      "[35]\tvalid_0's binary_logloss: 0.130715\n",
      "[36]\tvalid_0's binary_logloss: 0.129698\n",
      "[37]\tvalid_0's binary_logloss: 0.128727\n",
      "[38]\tvalid_0's binary_logloss: 0.127778\n",
      "[39]\tvalid_0's binary_logloss: 0.126871\n",
      "[40]\tvalid_0's binary_logloss: 0.125984\n",
      "[41]\tvalid_0's binary_logloss: 0.125136\n",
      "[42]\tvalid_0's binary_logloss: 0.124305\n",
      "[43]\tvalid_0's binary_logloss: 0.123511\n",
      "[44]\tvalid_0's binary_logloss: 0.122733\n",
      "[45]\tvalid_0's binary_logloss: 0.121989\n",
      "[46]\tvalid_0's binary_logloss: 0.121258\n",
      "[47]\tvalid_0's binary_logloss: 0.12056\n",
      "[48]\tvalid_0's binary_logloss: 0.119874\n",
      "[49]\tvalid_0's binary_logloss: 0.119219\n",
      "[50]\tvalid_0's binary_logloss: 0.118574\n",
      "[51]\tvalid_0's binary_logloss: 0.117958\n",
      "[52]\tvalid_0's binary_logloss: 0.117352\n",
      "[53]\tvalid_0's binary_logloss: 0.116772\n",
      "[54]\tvalid_0's binary_logloss: 0.116202\n",
      "[55]\tvalid_0's binary_logloss: 0.115656\n",
      "[56]\tvalid_0's binary_logloss: 0.115119\n",
      "[57]\tvalid_0's binary_logloss: 0.114605\n",
      "[58]\tvalid_0's binary_logloss: 0.114099\n",
      "[59]\tvalid_0's binary_logloss: 0.113615\n",
      "[60]\tvalid_0's binary_logloss: 0.113138\n",
      "[61]\tvalid_0's binary_logloss: 0.112681\n",
      "[62]\tvalid_0's binary_logloss: 0.112232\n",
      "[63]\tvalid_0's binary_logloss: 0.111801\n",
      "[64]\tvalid_0's binary_logloss: 0.111376\n",
      "[65]\tvalid_0's binary_logloss: 0.11097\n",
      "[66]\tvalid_0's binary_logloss: 0.110569\n",
      "[67]\tvalid_0's binary_logloss: 0.110185\n",
      "[68]\tvalid_0's binary_logloss: 0.109807\n",
      "[69]\tvalid_0's binary_logloss: 0.109444\n",
      "[70]\tvalid_0's binary_logloss: 0.109087\n",
      "[71]\tvalid_0's binary_logloss: 0.108745\n",
      "[72]\tvalid_0's binary_logloss: 0.108407\n",
      "[73]\tvalid_0's binary_logloss: 0.108083\n",
      "[74]\tvalid_0's binary_logloss: 0.107764\n",
      "[75]\tvalid_0's binary_logloss: 0.107458\n",
      "[76]\tvalid_0's binary_logloss: 0.107156\n",
      "[77]\tvalid_0's binary_logloss: 0.106867\n",
      "[78]\tvalid_0's binary_logloss: 0.106581\n",
      "[79]\tvalid_0's binary_logloss: 0.106308\n",
      "[80]\tvalid_0's binary_logloss: 0.106038\n",
      "[81]\tvalid_0's binary_logloss: 0.105779\n",
      "[82]\tvalid_0's binary_logloss: 0.105523\n",
      "[83]\tvalid_0's binary_logloss: 0.105279\n",
      "[84]\tvalid_0's binary_logloss: 0.105037\n",
      "[85]\tvalid_0's binary_logloss: 0.104805\n",
      "[86]\tvalid_0's binary_logloss: 0.104576\n",
      "[87]\tvalid_0's binary_logloss: 0.104357\n",
      "[88]\tvalid_0's binary_logloss: 0.10414\n",
      "[89]\tvalid_0's binary_logloss: 0.103933\n",
      "[90]\tvalid_0's binary_logloss: 0.103728\n",
      "[91]\tvalid_0's binary_logloss: 0.103531\n",
      "[92]\tvalid_0's binary_logloss: 0.103337\n",
      "[93]\tvalid_0's binary_logloss: 0.103151\n",
      "[94]\tvalid_0's binary_logloss: 0.102967\n",
      "[95]\tvalid_0's binary_logloss: 0.102791\n",
      "[96]\tvalid_0's binary_logloss: 0.102617\n",
      "[97]\tvalid_0's binary_logloss: 0.10245\n",
      "[98]\tvalid_0's binary_logloss: 0.102285\n",
      "[99]\tvalid_0's binary_logloss: 0.102126\n",
      "[100]\tvalid_0's binary_logloss: 0.10197\n",
      "[101]\tvalid_0's binary_logloss: 0.10182\n",
      "[102]\tvalid_0's binary_logloss: 0.101673\n",
      "[103]\tvalid_0's binary_logloss: 0.10153\n",
      "[104]\tvalid_0's binary_logloss: 0.10139\n",
      "[105]\tvalid_0's binary_logloss: 0.101256\n",
      "[106]\tvalid_0's binary_logloss: 0.101123\n",
      "[107]\tvalid_0's binary_logloss: 0.100995\n",
      "[108]\tvalid_0's binary_logloss: 0.100869\n",
      "[109]\tvalid_0's binary_logloss: 0.100748\n",
      "[110]\tvalid_0's binary_logloss: 0.100629\n",
      "[111]\tvalid_0's binary_logloss: 0.100515\n",
      "[112]\tvalid_0's binary_logloss: 0.100402\n",
      "[113]\tvalid_0's binary_logloss: 0.100293\n",
      "[114]\tvalid_0's binary_logloss: 0.100186\n",
      "[115]\tvalid_0's binary_logloss: 0.100083\n",
      "[116]\tvalid_0's binary_logloss: 0.0999818\n",
      "[117]\tvalid_0's binary_logloss: 0.0998842\n",
      "[118]\tvalid_0's binary_logloss: 0.0997881\n",
      "[119]\tvalid_0's binary_logloss: 0.0996956\n",
      "[120]\tvalid_0's binary_logloss: 0.0996045\n",
      "[121]\tvalid_0's binary_logloss: 0.0995168\n",
      "[122]\tvalid_0's binary_logloss: 0.0994305\n",
      "[123]\tvalid_0's binary_logloss: 0.0993473\n",
      "[124]\tvalid_0's binary_logloss: 0.0992655\n",
      "[125]\tvalid_0's binary_logloss: 0.0991866\n",
      "[126]\tvalid_0's binary_logloss: 0.0991091\n",
      "[127]\tvalid_0's binary_logloss: 0.0990344\n",
      "[128]\tvalid_0's binary_logloss: 0.0989609\n",
      "[129]\tvalid_0's binary_logloss: 0.0988901\n",
      "[130]\tvalid_0's binary_logloss: 0.0988205\n",
      "[131]\tvalid_0's binary_logloss: 0.0987533\n",
      "[132]\tvalid_0's binary_logloss: 0.0986874\n",
      "[133]\tvalid_0's binary_logloss: 0.0986236\n",
      "[134]\tvalid_0's binary_logloss: 0.098561\n",
      "[135]\tvalid_0's binary_logloss: 0.0985005\n",
      "[136]\tvalid_0's binary_logloss: 0.0984413\n",
      "[137]\tvalid_0's binary_logloss: 0.0983839\n",
      "[138]\tvalid_0's binary_logloss: 0.0983277\n",
      "[139]\tvalid_0's binary_logloss: 0.0982733\n",
      "[140]\tvalid_0's binary_logloss: 0.0982201\n",
      "[141]\tvalid_0's binary_logloss: 0.0981687\n",
      "[142]\tvalid_0's binary_logloss: 0.0981182\n",
      "[143]\tvalid_0's binary_logloss: 0.0980694\n",
      "[144]\tvalid_0's binary_logloss: 0.0980217\n",
      "[145]\tvalid_0's binary_logloss: 0.0979755\n",
      "[146]\tvalid_0's binary_logloss: 0.0979303\n",
      "[147]\tvalid_0's binary_logloss: 0.0978863\n",
      "[148]\tvalid_0's binary_logloss: 0.0978435\n",
      "[149]\tvalid_0's binary_logloss: 0.0978019\n",
      "[150]\tvalid_0's binary_logloss: 0.0977615\n",
      "[151]\tvalid_0's binary_logloss: 0.097722\n",
      "[152]\tvalid_0's binary_logloss: 0.0976837\n",
      "[153]\tvalid_0's binary_logloss: 0.0976463\n",
      "[154]\tvalid_0's binary_logloss: 0.0976099\n",
      "[155]\tvalid_0's binary_logloss: 0.0975745\n",
      "[156]\tvalid_0's binary_logloss: 0.0975402\n",
      "[157]\tvalid_0's binary_logloss: 0.0975066\n",
      "[158]\tvalid_0's binary_logloss: 0.0974739\n",
      "[159]\tvalid_0's binary_logloss: 0.0974421\n",
      "[160]\tvalid_0's binary_logloss: 0.0974112\n",
      "[161]\tvalid_0's binary_logloss: 0.0973811\n",
      "[162]\tvalid_0's binary_logloss: 0.0973519\n",
      "[163]\tvalid_0's binary_logloss: 0.0973233\n",
      "[164]\tvalid_0's binary_logloss: 0.0972955\n",
      "[165]\tvalid_0's binary_logloss: 0.0972684\n",
      "[166]\tvalid_0's binary_logloss: 0.0972422\n",
      "[167]\tvalid_0's binary_logloss: 0.0972167\n",
      "[168]\tvalid_0's binary_logloss: 0.0971919\n",
      "[169]\tvalid_0's binary_logloss: 0.0971677\n",
      "[170]\tvalid_0's binary_logloss: 0.0971443\n",
      "[171]\tvalid_0's binary_logloss: 0.0971214\n",
      "[172]\tvalid_0's binary_logloss: 0.0970992\n",
      "[173]\tvalid_0's binary_logloss: 0.0970774\n",
      "[174]\tvalid_0's binary_logloss: 0.0970565\n",
      "[175]\tvalid_0's binary_logloss: 0.0970358\n",
      "[176]\tvalid_0's binary_logloss: 0.097016\n",
      "[177]\tvalid_0's binary_logloss: 0.0969965\n",
      "[178]\tvalid_0's binary_logloss: 0.0969777\n",
      "[179]\tvalid_0's binary_logloss: 0.0969591\n",
      "[180]\tvalid_0's binary_logloss: 0.0969413\n",
      "[181]\tvalid_0's binary_logloss: 0.0969239\n",
      "[182]\tvalid_0's binary_logloss: 0.096907\n",
      "[183]\tvalid_0's binary_logloss: 0.0968904\n",
      "[184]\tvalid_0's binary_logloss: 0.0968746\n",
      "[185]\tvalid_0's binary_logloss: 0.0968589\n",
      "[186]\tvalid_0's binary_logloss: 0.0968439\n",
      "[187]\tvalid_0's binary_logloss: 0.0968291\n",
      "[188]\tvalid_0's binary_logloss: 0.0968149\n",
      "[189]\tvalid_0's binary_logloss: 0.0968009\n",
      "[190]\tvalid_0's binary_logloss: 0.0967875\n",
      "[191]\tvalid_0's binary_logloss: 0.0967741\n",
      "[192]\tvalid_0's binary_logloss: 0.0967615\n",
      "[193]\tvalid_0's binary_logloss: 0.0967488\n",
      "[194]\tvalid_0's binary_logloss: 0.0967368\n",
      "[195]\tvalid_0's binary_logloss: 0.096725\n",
      "[196]\tvalid_0's binary_logloss: 0.0967138\n",
      "[197]\tvalid_0's binary_logloss: 0.0967026\n",
      "[198]\tvalid_0's binary_logloss: 0.096692\n",
      "[199]\tvalid_0's binary_logloss: 0.0966813\n",
      "[200]\tvalid_0's binary_logloss: 0.0966713\n",
      "[201]\tvalid_0's binary_logloss: 0.0966612\n",
      "[202]\tvalid_0's binary_logloss: 0.0966518\n",
      "[203]\tvalid_0's binary_logloss: 0.0966423\n",
      "[204]\tvalid_0's binary_logloss: 0.0966333\n",
      "[205]\tvalid_0's binary_logloss: 0.0966244\n",
      "[206]\tvalid_0's binary_logloss: 0.096616\n",
      "[207]\tvalid_0's binary_logloss: 0.0966075\n",
      "[208]\tvalid_0's binary_logloss: 0.0965996\n",
      "[209]\tvalid_0's binary_logloss: 0.0965918\n",
      "[210]\tvalid_0's binary_logloss: 0.0965844\n",
      "[211]\tvalid_0's binary_logloss: 0.0965769\n",
      "[212]\tvalid_0's binary_logloss: 0.0965701\n",
      "[213]\tvalid_0's binary_logloss: 0.0965631\n",
      "[214]\tvalid_0's binary_logloss: 0.0965565\n",
      "[215]\tvalid_0's binary_logloss: 0.0965499\n",
      "[216]\tvalid_0's binary_logloss: 0.0965437\n",
      "[217]\tvalid_0's binary_logloss: 0.0965375\n",
      "[218]\tvalid_0's binary_logloss: 0.0965317\n",
      "[219]\tvalid_0's binary_logloss: 0.0965259\n",
      "[220]\tvalid_0's binary_logloss: 0.0965205\n",
      "[221]\tvalid_0's binary_logloss: 0.0965149\n",
      "[222]\tvalid_0's binary_logloss: 0.0965098\n",
      "[223]\tvalid_0's binary_logloss: 0.0965046\n",
      "[224]\tvalid_0's binary_logloss: 0.0964998\n",
      "[225]\tvalid_0's binary_logloss: 0.096495\n",
      "[226]\tvalid_0's binary_logloss: 0.0964905\n",
      "[227]\tvalid_0's binary_logloss: 0.096486\n",
      "[228]\tvalid_0's binary_logloss: 0.0964818\n",
      "[229]\tvalid_0's binary_logloss: 0.0964775\n",
      "[230]\tvalid_0's binary_logloss: 0.0964736\n",
      "[231]\tvalid_0's binary_logloss: 0.0964696\n",
      "[232]\tvalid_0's binary_logloss: 0.0964659\n",
      "[233]\tvalid_0's binary_logloss: 0.0964622\n",
      "[234]\tvalid_0's binary_logloss: 0.0964588\n",
      "[235]\tvalid_0's binary_logloss: 0.0964553\n",
      "[236]\tvalid_0's binary_logloss: 0.0964521\n",
      "[237]\tvalid_0's binary_logloss: 0.0964487\n",
      "[238]\tvalid_0's binary_logloss: 0.0964457\n",
      "[239]\tvalid_0's binary_logloss: 0.0964426\n",
      "[240]\tvalid_0's binary_logloss: 0.0964398\n",
      "[241]\tvalid_0's binary_logloss: 0.0964369\n",
      "[242]\tvalid_0's binary_logloss: 0.0964343\n",
      "[243]\tvalid_0's binary_logloss: 0.0964314\n",
      "[244]\tvalid_0's binary_logloss: 0.096429\n",
      "[245]\tvalid_0's binary_logloss: 0.0964264\n",
      "[246]\tvalid_0's binary_logloss: 0.0964242\n",
      "[247]\tvalid_0's binary_logloss: 0.0964216\n",
      "[248]\tvalid_0's binary_logloss: 0.0964195\n",
      "[249]\tvalid_0's binary_logloss: 0.0964172\n",
      "[250]\tvalid_0's binary_logloss: 0.0964153\n",
      "[251]\tvalid_0's binary_logloss: 0.096413\n",
      "[252]\tvalid_0's binary_logloss: 0.0964113\n",
      "[253]\tvalid_0's binary_logloss: 0.0964093\n",
      "[254]\tvalid_0's binary_logloss: 0.0964076\n",
      "[255]\tvalid_0's binary_logloss: 0.0964057\n",
      "[256]\tvalid_0's binary_logloss: 0.0964041\n",
      "[257]\tvalid_0's binary_logloss: 0.0964024\n",
      "[258]\tvalid_0's binary_logloss: 0.0964009\n",
      "[259]\tvalid_0's binary_logloss: 0.0963992\n",
      "[260]\tvalid_0's binary_logloss: 0.0963978\n",
      "[261]\tvalid_0's binary_logloss: 0.0963962\n",
      "[262]\tvalid_0's binary_logloss: 0.0963949\n",
      "[263]\tvalid_0's binary_logloss: 0.0963935\n",
      "[264]\tvalid_0's binary_logloss: 0.0963924\n",
      "[265]\tvalid_0's binary_logloss: 0.096391\n",
      "[266]\tvalid_0's binary_logloss: 0.09639\n",
      "[267]\tvalid_0's binary_logloss: 0.0963887\n",
      "[268]\tvalid_0's binary_logloss: 0.0963878\n",
      "[269]\tvalid_0's binary_logloss: 0.0963867\n",
      "[270]\tvalid_0's binary_logloss: 0.0963858\n",
      "[271]\tvalid_0's binary_logloss: 0.0963849\n",
      "[272]\tvalid_0's binary_logloss: 0.0963841\n",
      "[273]\tvalid_0's binary_logloss: 0.0963831\n",
      "[274]\tvalid_0's binary_logloss: 0.0963824\n",
      "[275]\tvalid_0's binary_logloss: 0.0963815\n",
      "[276]\tvalid_0's binary_logloss: 0.096381\n",
      "[277]\tvalid_0's binary_logloss: 0.0963802\n",
      "[278]\tvalid_0's binary_logloss: 0.0963795\n",
      "[279]\tvalid_0's binary_logloss: 0.0963788\n",
      "[280]\tvalid_0's binary_logloss: 0.0963785\n",
      "[281]\tvalid_0's binary_logloss: 0.0963778\n",
      "[282]\tvalid_0's binary_logloss: 0.0963774\n",
      "[283]\tvalid_0's binary_logloss: 0.0963768\n",
      "[284]\tvalid_0's binary_logloss: 0.0963764\n",
      "[285]\tvalid_0's binary_logloss: 0.096376\n",
      "[286]\tvalid_0's binary_logloss: 0.0963758\n",
      "[287]\tvalid_0's binary_logloss: 0.0963753\n",
      "[288]\tvalid_0's binary_logloss: 0.0963751\n",
      "[289]\tvalid_0's binary_logloss: 0.0963747\n",
      "[290]\tvalid_0's binary_logloss: 0.0963746\n",
      "[291]\tvalid_0's binary_logloss: 0.0963743\n",
      "[292]\tvalid_0's binary_logloss: 0.0963742\n",
      "[293]\tvalid_0's binary_logloss: 0.0963739\n",
      "[294]\tvalid_0's binary_logloss: 0.0963739\n",
      "[295]\tvalid_0's binary_logloss: 0.0963737\n",
      "[296]\tvalid_0's binary_logloss: 0.0963738\n",
      "[297]\tvalid_0's binary_logloss: 0.0963737\n",
      "[298]\tvalid_0's binary_logloss: 0.0963737\n",
      "[299]\tvalid_0's binary_logloss: 0.0963736\n",
      "[300]\tvalid_0's binary_logloss: 0.0963738\n",
      "[301]\tvalid_0's binary_logloss: 0.0963734\n",
      "[302]\tvalid_0's binary_logloss: 0.0963736\n",
      "[303]\tvalid_0's binary_logloss: 0.0963735\n",
      "[304]\tvalid_0's binary_logloss: 0.0963736\n",
      "[305]\tvalid_0's binary_logloss: 0.0963733\n",
      "[306]\tvalid_0's binary_logloss: 0.0963735\n",
      "[307]\tvalid_0's binary_logloss: 0.0963734\n",
      "[308]\tvalid_0's binary_logloss: 0.0963736\n",
      "[309]\tvalid_0's binary_logloss: 0.0963736\n",
      "[310]\tvalid_0's binary_logloss: 0.0963738\n",
      "[311]\tvalid_0's binary_logloss: 0.0963737\n",
      "[312]\tvalid_0's binary_logloss: 0.0963739\n",
      "[313]\tvalid_0's binary_logloss: 0.096374\n",
      "[314]\tvalid_0's binary_logloss: 0.0963744\n",
      "[315]\tvalid_0's binary_logloss: 0.0963745\n",
      "[316]\tvalid_0's binary_logloss: 0.0963748\n",
      "[317]\tvalid_0's binary_logloss: 0.0963748\n",
      "[318]\tvalid_0's binary_logloss: 0.0963751\n",
      "[319]\tvalid_0's binary_logloss: 0.0963752\n",
      "[320]\tvalid_0's binary_logloss: 0.0963755\n",
      "[321]\tvalid_0's binary_logloss: 0.0963756\n",
      "[322]\tvalid_0's binary_logloss: 0.0963759\n",
      "[323]\tvalid_0's binary_logloss: 0.0963761\n",
      "[324]\tvalid_0's binary_logloss: 0.0963763\n",
      "[325]\tvalid_0's binary_logloss: 0.0963764\n",
      "Early stopping, best iteration is:\n",
      "[305]\tvalid_0's binary_logloss: 0.0963733\n",
      "2nd level Fold 4\n",
      "[1]\tvalid_0's binary_logloss: 0.222804\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.214722\n",
      "[3]\tvalid_0's binary_logloss: 0.208195\n",
      "[4]\tvalid_0's binary_logloss: 0.202217\n",
      "[5]\tvalid_0's binary_logloss: 0.197123\n",
      "[6]\tvalid_0's binary_logloss: 0.192348\n",
      "[7]\tvalid_0's binary_logloss: 0.188158\n",
      "[8]\tvalid_0's binary_logloss: 0.184179\n",
      "[9]\tvalid_0's binary_logloss: 0.180621\n",
      "[10]\tvalid_0's binary_logloss: 0.177211\n",
      "[11]\tvalid_0's binary_logloss: 0.174124\n",
      "[12]\tvalid_0's binary_logloss: 0.171147\n",
      "[13]\tvalid_0's binary_logloss: 0.168427\n",
      "[14]\tvalid_0's binary_logloss: 0.165791\n",
      "[15]\tvalid_0's binary_logloss: 0.163365\n",
      "[16]\tvalid_0's binary_logloss: 0.161006\n",
      "[17]\tvalid_0's binary_logloss: 0.158823\n",
      "[18]\tvalid_0's binary_logloss: 0.156695\n",
      "[19]\tvalid_0's binary_logloss: 0.154717\n",
      "[20]\tvalid_0's binary_logloss: 0.152783\n",
      "[21]\tvalid_0's binary_logloss: 0.15098\n",
      "[22]\tvalid_0's binary_logloss: 0.149213\n",
      "[23]\tvalid_0's binary_logloss: 0.147562\n",
      "[24]\tvalid_0's binary_logloss: 0.145941\n",
      "[25]\tvalid_0's binary_logloss: 0.144422\n",
      "[26]\tvalid_0's binary_logloss: 0.142928\n",
      "[27]\tvalid_0's binary_logloss: 0.141527\n",
      "[28]\tvalid_0's binary_logloss: 0.140147\n",
      "[29]\tvalid_0's binary_logloss: 0.138849\n",
      "[30]\tvalid_0's binary_logloss: 0.13757\n",
      "[31]\tvalid_0's binary_logloss: 0.136366\n",
      "[32]\tvalid_0's binary_logloss: 0.135178\n",
      "[33]\tvalid_0's binary_logloss: 0.134058\n",
      "[34]\tvalid_0's binary_logloss: 0.132952\n",
      "[35]\tvalid_0's binary_logloss: 0.131908\n",
      "[36]\tvalid_0's binary_logloss: 0.130876\n",
      "[37]\tvalid_0's binary_logloss: 0.129902\n",
      "[38]\tvalid_0's binary_logloss: 0.128938\n",
      "[39]\tvalid_0's binary_logloss: 0.128028\n",
      "[40]\tvalid_0's binary_logloss: 0.127126\n",
      "[41]\tvalid_0's binary_logloss: 0.126273\n",
      "[42]\tvalid_0's binary_logloss: 0.125429\n",
      "[43]\tvalid_0's binary_logloss: 0.12463\n",
      "[44]\tvalid_0's binary_logloss: 0.123838\n",
      "[45]\tvalid_0's binary_logloss: 0.123088\n",
      "[46]\tvalid_0's binary_logloss: 0.122345\n",
      "[47]\tvalid_0's binary_logloss: 0.12164\n",
      "[48]\tvalid_0's binary_logloss: 0.120942\n",
      "[49]\tvalid_0's binary_logloss: 0.12028\n",
      "[50]\tvalid_0's binary_logloss: 0.119623\n",
      "[51]\tvalid_0's binary_logloss: 0.119\n",
      "[52]\tvalid_0's binary_logloss: 0.118382\n",
      "[53]\tvalid_0's binary_logloss: 0.117796\n",
      "[54]\tvalid_0's binary_logloss: 0.117213\n",
      "[55]\tvalid_0's binary_logloss: 0.116661\n",
      "[56]\tvalid_0's binary_logloss: 0.116112\n",
      "[57]\tvalid_0's binary_logloss: 0.115592\n",
      "[58]\tvalid_0's binary_logloss: 0.115074\n",
      "[59]\tvalid_0's binary_logloss: 0.114584\n",
      "[60]\tvalid_0's binary_logloss: 0.114095\n",
      "[61]\tvalid_0's binary_logloss: 0.113632\n",
      "[62]\tvalid_0's binary_logloss: 0.113172\n",
      "[63]\tvalid_0's binary_logloss: 0.112734\n",
      "[64]\tvalid_0's binary_logloss: 0.112299\n",
      "[65]\tvalid_0's binary_logloss: 0.111886\n",
      "[66]\tvalid_0's binary_logloss: 0.111476\n",
      "[67]\tvalid_0's binary_logloss: 0.111085\n",
      "[68]\tvalid_0's binary_logloss: 0.110697\n",
      "[69]\tvalid_0's binary_logloss: 0.110328\n",
      "[70]\tvalid_0's binary_logloss: 0.109961\n",
      "[71]\tvalid_0's binary_logloss: 0.109613\n",
      "[72]\tvalid_0's binary_logloss: 0.109266\n",
      "[73]\tvalid_0's binary_logloss: 0.108936\n",
      "[74]\tvalid_0's binary_logloss: 0.108608\n",
      "[75]\tvalid_0's binary_logloss: 0.108296\n",
      "[76]\tvalid_0's binary_logloss: 0.107985\n",
      "[77]\tvalid_0's binary_logloss: 0.10769\n",
      "[78]\tvalid_0's binary_logloss: 0.107396\n",
      "[79]\tvalid_0's binary_logloss: 0.107117\n",
      "[80]\tvalid_0's binary_logloss: 0.106839\n",
      "[81]\tvalid_0's binary_logloss: 0.106575\n",
      "[82]\tvalid_0's binary_logloss: 0.106311\n",
      "[83]\tvalid_0's binary_logloss: 0.106061\n",
      "[84]\tvalid_0's binary_logloss: 0.105812\n",
      "[85]\tvalid_0's binary_logloss: 0.105575\n",
      "[86]\tvalid_0's binary_logloss: 0.105339\n",
      "[87]\tvalid_0's binary_logloss: 0.105115\n",
      "[88]\tvalid_0's binary_logloss: 0.104891\n",
      "[89]\tvalid_0's binary_logloss: 0.104678\n",
      "[90]\tvalid_0's binary_logloss: 0.104467\n",
      "[91]\tvalid_0's binary_logloss: 0.104265\n",
      "[92]\tvalid_0's binary_logloss: 0.104065\n",
      "[93]\tvalid_0's binary_logloss: 0.103874\n",
      "[94]\tvalid_0's binary_logloss: 0.103684\n",
      "[95]\tvalid_0's binary_logloss: 0.103503\n",
      "[96]\tvalid_0's binary_logloss: 0.103323\n",
      "[97]\tvalid_0's binary_logloss: 0.103152\n",
      "[98]\tvalid_0's binary_logloss: 0.102981\n",
      "[99]\tvalid_0's binary_logloss: 0.102819\n",
      "[100]\tvalid_0's binary_logloss: 0.102657\n",
      "[101]\tvalid_0's binary_logloss: 0.102503\n",
      "[102]\tvalid_0's binary_logloss: 0.10235\n",
      "[103]\tvalid_0's binary_logloss: 0.102204\n",
      "[104]\tvalid_0's binary_logloss: 0.102059\n",
      "[105]\tvalid_0's binary_logloss: 0.101921\n",
      "[106]\tvalid_0's binary_logloss: 0.101783\n",
      "[107]\tvalid_0's binary_logloss: 0.101652\n",
      "[108]\tvalid_0's binary_logloss: 0.101522\n",
      "[109]\tvalid_0's binary_logloss: 0.101397\n",
      "[110]\tvalid_0's binary_logloss: 0.101274\n",
      "[111]\tvalid_0's binary_logloss: 0.101156\n",
      "[112]\tvalid_0's binary_logloss: 0.101039\n",
      "[113]\tvalid_0's binary_logloss: 0.100927\n",
      "[114]\tvalid_0's binary_logloss: 0.100816\n",
      "[115]\tvalid_0's binary_logloss: 0.10071\n",
      "[116]\tvalid_0's binary_logloss: 0.100605\n",
      "[117]\tvalid_0's binary_logloss: 0.100504\n",
      "[118]\tvalid_0's binary_logloss: 0.100405\n",
      "[119]\tvalid_0's binary_logloss: 0.100309\n",
      "[120]\tvalid_0's binary_logloss: 0.100215\n",
      "[121]\tvalid_0's binary_logloss: 0.100124\n",
      "[122]\tvalid_0's binary_logloss: 0.100035\n",
      "[123]\tvalid_0's binary_logloss: 0.0999489\n",
      "[124]\tvalid_0's binary_logloss: 0.0998642\n",
      "[125]\tvalid_0's binary_logloss: 0.0997828\n",
      "[126]\tvalid_0's binary_logloss: 0.0997025\n",
      "[127]\tvalid_0's binary_logloss: 0.0996253\n",
      "[128]\tvalid_0's binary_logloss: 0.0995492\n",
      "[129]\tvalid_0's binary_logloss: 0.0994759\n",
      "[130]\tvalid_0's binary_logloss: 0.0994039\n",
      "[131]\tvalid_0's binary_logloss: 0.0993344\n",
      "[132]\tvalid_0's binary_logloss: 0.0992661\n",
      "[133]\tvalid_0's binary_logloss: 0.0992002\n",
      "[134]\tvalid_0's binary_logloss: 0.0991355\n",
      "[135]\tvalid_0's binary_logloss: 0.0990731\n",
      "[136]\tvalid_0's binary_logloss: 0.0990118\n",
      "[137]\tvalid_0's binary_logloss: 0.0989526\n",
      "[138]\tvalid_0's binary_logloss: 0.0988945\n",
      "[139]\tvalid_0's binary_logloss: 0.0988383\n",
      "[140]\tvalid_0's binary_logloss: 0.0987834\n",
      "[141]\tvalid_0's binary_logloss: 0.0987301\n",
      "[142]\tvalid_0's binary_logloss: 0.0986781\n",
      "[143]\tvalid_0's binary_logloss: 0.0986276\n",
      "[144]\tvalid_0's binary_logloss: 0.0985783\n",
      "[145]\tvalid_0's binary_logloss: 0.0985305\n",
      "[146]\tvalid_0's binary_logloss: 0.0984839\n",
      "[147]\tvalid_0's binary_logloss: 0.0984385\n",
      "[148]\tvalid_0's binary_logloss: 0.0983943\n",
      "[149]\tvalid_0's binary_logloss: 0.0983514\n",
      "[150]\tvalid_0's binary_logloss: 0.0983095\n",
      "[151]\tvalid_0's binary_logloss: 0.0982689\n",
      "[152]\tvalid_0's binary_logloss: 0.0982292\n",
      "[153]\tvalid_0's binary_logloss: 0.0981907\n",
      "[154]\tvalid_0's binary_logloss: 0.0981532\n",
      "[155]\tvalid_0's binary_logloss: 0.0981167\n",
      "[156]\tvalid_0's binary_logloss: 0.0980812\n",
      "[157]\tvalid_0's binary_logloss: 0.0980466\n",
      "[158]\tvalid_0's binary_logloss: 0.0980131\n",
      "[159]\tvalid_0's binary_logloss: 0.0979805\n",
      "[160]\tvalid_0's binary_logloss: 0.0979488\n",
      "[161]\tvalid_0's binary_logloss: 0.0979179\n",
      "[162]\tvalid_0's binary_logloss: 0.0978878\n",
      "[163]\tvalid_0's binary_logloss: 0.0978587\n",
      "[164]\tvalid_0's binary_logloss: 0.0978302\n",
      "[165]\tvalid_0's binary_logloss: 0.0978025\n",
      "[166]\tvalid_0's binary_logloss: 0.0977757\n",
      "[167]\tvalid_0's binary_logloss: 0.0977495\n",
      "[168]\tvalid_0's binary_logloss: 0.0977242\n",
      "[169]\tvalid_0's binary_logloss: 0.0976995\n",
      "[170]\tvalid_0's binary_logloss: 0.0976756\n",
      "[171]\tvalid_0's binary_logloss: 0.097652\n",
      "[172]\tvalid_0's binary_logloss: 0.0976294\n",
      "[173]\tvalid_0's binary_logloss: 0.0976071\n",
      "[174]\tvalid_0's binary_logloss: 0.0975857\n",
      "[175]\tvalid_0's binary_logloss: 0.0975647\n",
      "[176]\tvalid_0's binary_logloss: 0.0975445\n",
      "[177]\tvalid_0's binary_logloss: 0.0975246\n",
      "[178]\tvalid_0's binary_logloss: 0.0975055\n",
      "[179]\tvalid_0's binary_logloss: 0.0974867\n",
      "[180]\tvalid_0's binary_logloss: 0.0974688\n",
      "[181]\tvalid_0's binary_logloss: 0.0974511\n",
      "[182]\tvalid_0's binary_logloss: 0.0974342\n",
      "[183]\tvalid_0's binary_logloss: 0.0974173\n",
      "[184]\tvalid_0's binary_logloss: 0.0974012\n",
      "[185]\tvalid_0's binary_logloss: 0.0973853\n",
      "[186]\tvalid_0's binary_logloss: 0.0973703\n",
      "[187]\tvalid_0's binary_logloss: 0.0973553\n",
      "[188]\tvalid_0's binary_logloss: 0.0973411\n",
      "[189]\tvalid_0's binary_logloss: 0.097327\n",
      "[190]\tvalid_0's binary_logloss: 0.0973135\n",
      "[191]\tvalid_0's binary_logloss: 0.0973002\n",
      "[192]\tvalid_0's binary_logloss: 0.0972876\n",
      "[193]\tvalid_0's binary_logloss: 0.097275\n",
      "[194]\tvalid_0's binary_logloss: 0.097263\n",
      "[195]\tvalid_0's binary_logloss: 0.0972512\n",
      "[196]\tvalid_0's binary_logloss: 0.09724\n",
      "[197]\tvalid_0's binary_logloss: 0.0972288\n",
      "[198]\tvalid_0's binary_logloss: 0.0972182\n",
      "[199]\tvalid_0's binary_logloss: 0.0972077\n",
      "[200]\tvalid_0's binary_logloss: 0.0971978\n",
      "[201]\tvalid_0's binary_logloss: 0.0971878\n",
      "[202]\tvalid_0's binary_logloss: 0.0971786\n",
      "[203]\tvalid_0's binary_logloss: 0.0971694\n",
      "[204]\tvalid_0's binary_logloss: 0.0971607\n",
      "[205]\tvalid_0's binary_logloss: 0.0971519\n",
      "[206]\tvalid_0's binary_logloss: 0.0971438\n",
      "[207]\tvalid_0's binary_logloss: 0.0971355\n",
      "[208]\tvalid_0's binary_logloss: 0.0971279\n",
      "[209]\tvalid_0's binary_logloss: 0.0971199\n",
      "[210]\tvalid_0's binary_logloss: 0.0971128\n",
      "[211]\tvalid_0's binary_logloss: 0.0971054\n",
      "[212]\tvalid_0's binary_logloss: 0.0970988\n",
      "[213]\tvalid_0's binary_logloss: 0.0970918\n",
      "[214]\tvalid_0's binary_logloss: 0.0970856\n",
      "[215]\tvalid_0's binary_logloss: 0.097079\n",
      "[216]\tvalid_0's binary_logloss: 0.0970732\n",
      "[217]\tvalid_0's binary_logloss: 0.0970671\n",
      "[218]\tvalid_0's binary_logloss: 0.0970617\n",
      "[219]\tvalid_0's binary_logloss: 0.0970559\n",
      "[220]\tvalid_0's binary_logloss: 0.0970507\n",
      "[221]\tvalid_0's binary_logloss: 0.0970453\n",
      "[222]\tvalid_0's binary_logloss: 0.0970406\n",
      "[223]\tvalid_0's binary_logloss: 0.0970355\n",
      "[224]\tvalid_0's binary_logloss: 0.0970311\n",
      "[225]\tvalid_0's binary_logloss: 0.0970265\n",
      "[226]\tvalid_0's binary_logloss: 0.0970224\n",
      "[227]\tvalid_0's binary_logloss: 0.097018\n",
      "[228]\tvalid_0's binary_logloss: 0.0970141\n",
      "[229]\tvalid_0's binary_logloss: 0.09701\n",
      "[230]\tvalid_0's binary_logloss: 0.0970064\n",
      "[231]\tvalid_0's binary_logloss: 0.0970026\n",
      "[232]\tvalid_0's binary_logloss: 0.0969992\n",
      "[233]\tvalid_0's binary_logloss: 0.0969956\n",
      "[234]\tvalid_0's binary_logloss: 0.0969924\n",
      "[235]\tvalid_0's binary_logloss: 0.0969891\n",
      "[236]\tvalid_0's binary_logloss: 0.0969862\n",
      "[237]\tvalid_0's binary_logloss: 0.096983\n",
      "[238]\tvalid_0's binary_logloss: 0.0969803\n",
      "[239]\tvalid_0's binary_logloss: 0.0969774\n",
      "[240]\tvalid_0's binary_logloss: 0.096975\n",
      "[241]\tvalid_0's binary_logloss: 0.0969723\n",
      "[242]\tvalid_0's binary_logloss: 0.0969701\n",
      "[243]\tvalid_0's binary_logloss: 0.0969676\n",
      "[244]\tvalid_0's binary_logloss: 0.0969656\n",
      "[245]\tvalid_0's binary_logloss: 0.0969633\n",
      "[246]\tvalid_0's binary_logloss: 0.0969615\n",
      "[247]\tvalid_0's binary_logloss: 0.0969594\n",
      "[248]\tvalid_0's binary_logloss: 0.0969577\n",
      "[249]\tvalid_0's binary_logloss: 0.0969558\n",
      "[250]\tvalid_0's binary_logloss: 0.0969542\n",
      "[251]\tvalid_0's binary_logloss: 0.0969525\n",
      "[252]\tvalid_0's binary_logloss: 0.0969511\n",
      "[253]\tvalid_0's binary_logloss: 0.0969494\n",
      "[254]\tvalid_0's binary_logloss: 0.0969481\n",
      "[255]\tvalid_0's binary_logloss: 0.0969466\n",
      "[256]\tvalid_0's binary_logloss: 0.0969454\n",
      "[257]\tvalid_0's binary_logloss: 0.096944\n",
      "[258]\tvalid_0's binary_logloss: 0.096943\n",
      "[259]\tvalid_0's binary_logloss: 0.0969418\n",
      "[260]\tvalid_0's binary_logloss: 0.0969409\n",
      "[261]\tvalid_0's binary_logloss: 0.0969397\n",
      "[262]\tvalid_0's binary_logloss: 0.096939\n",
      "[263]\tvalid_0's binary_logloss: 0.096938\n",
      "[264]\tvalid_0's binary_logloss: 0.0969373\n",
      "[265]\tvalid_0's binary_logloss: 0.0969363\n",
      "[266]\tvalid_0's binary_logloss: 0.0969358\n",
      "[267]\tvalid_0's binary_logloss: 0.0969348\n",
      "[268]\tvalid_0's binary_logloss: 0.0969343\n",
      "[269]\tvalid_0's binary_logloss: 0.0969336\n",
      "[270]\tvalid_0's binary_logloss: 0.0969332\n",
      "[271]\tvalid_0's binary_logloss: 0.0969324\n",
      "[272]\tvalid_0's binary_logloss: 0.0969321\n",
      "[273]\tvalid_0's binary_logloss: 0.0969316\n",
      "[274]\tvalid_0's binary_logloss: 0.0969313\n",
      "[275]\tvalid_0's binary_logloss: 0.0969308\n",
      "[276]\tvalid_0's binary_logloss: 0.0969307\n",
      "[277]\tvalid_0's binary_logloss: 0.0969302\n",
      "[278]\tvalid_0's binary_logloss: 0.0969301\n",
      "[279]\tvalid_0's binary_logloss: 0.0969297\n",
      "[280]\tvalid_0's binary_logloss: 0.0969297\n",
      "[281]\tvalid_0's binary_logloss: 0.0969294\n",
      "[282]\tvalid_0's binary_logloss: 0.0969293\n",
      "[283]\tvalid_0's binary_logloss: 0.096929\n",
      "[284]\tvalid_0's binary_logloss: 0.0969292\n",
      "[285]\tvalid_0's binary_logloss: 0.096929\n",
      "[286]\tvalid_0's binary_logloss: 0.0969291\n",
      "[287]\tvalid_0's binary_logloss: 0.096929\n",
      "[288]\tvalid_0's binary_logloss: 0.0969292\n",
      "[289]\tvalid_0's binary_logloss: 0.0969292\n",
      "[290]\tvalid_0's binary_logloss: 0.0969294\n",
      "[291]\tvalid_0's binary_logloss: 0.0969293\n",
      "[292]\tvalid_0's binary_logloss: 0.0969297\n",
      "[293]\tvalid_0's binary_logloss: 0.0969296\n",
      "[294]\tvalid_0's binary_logloss: 0.0969299\n",
      "[295]\tvalid_0's binary_logloss: 0.0969298\n",
      "[296]\tvalid_0's binary_logloss: 0.0969302\n",
      "[297]\tvalid_0's binary_logloss: 0.0969301\n",
      "[298]\tvalid_0's binary_logloss: 0.0969304\n",
      "[299]\tvalid_0's binary_logloss: 0.0969305\n",
      "[300]\tvalid_0's binary_logloss: 0.0969309\n",
      "[301]\tvalid_0's binary_logloss: 0.0969309\n",
      "[302]\tvalid_0's binary_logloss: 0.0969313\n",
      "[303]\tvalid_0's binary_logloss: 0.0969313\n",
      "[304]\tvalid_0's binary_logloss: 0.0969317\n",
      "[305]\tvalid_0's binary_logloss: 0.0969317\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid_0's binary_logloss: 0.096929\n",
      "2nd level Fold 5\n",
      "[1]\tvalid_0's binary_logloss: 0.222278\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.214101\n",
      "[3]\tvalid_0's binary_logloss: 0.207284\n",
      "[4]\tvalid_0's binary_logloss: 0.201247\n",
      "[5]\tvalid_0's binary_logloss: 0.195959\n",
      "[6]\tvalid_0's binary_logloss: 0.191144\n",
      "[7]\tvalid_0's binary_logloss: 0.186813\n",
      "[8]\tvalid_0's binary_logloss: 0.182803\n",
      "[9]\tvalid_0's binary_logloss: 0.179137\n",
      "[10]\tvalid_0's binary_logloss: 0.175704\n",
      "[11]\tvalid_0's binary_logloss: 0.172531\n",
      "[12]\tvalid_0's binary_logloss: 0.169535\n",
      "[13]\tvalid_0's binary_logloss: 0.166745\n",
      "[14]\tvalid_0's binary_logloss: 0.164093\n",
      "[15]\tvalid_0's binary_logloss: 0.16161\n",
      "[16]\tvalid_0's binary_logloss: 0.159238\n",
      "[17]\tvalid_0's binary_logloss: 0.157007\n",
      "[18]\tvalid_0's binary_logloss: 0.154868\n",
      "[19]\tvalid_0's binary_logloss: 0.152849\n",
      "[20]\tvalid_0's binary_logloss: 0.150906\n",
      "[21]\tvalid_0's binary_logloss: 0.149068\n",
      "[22]\tvalid_0's binary_logloss: 0.147294\n",
      "[23]\tvalid_0's binary_logloss: 0.145611\n",
      "[24]\tvalid_0's binary_logloss: 0.143984\n",
      "[25]\tvalid_0's binary_logloss: 0.142438\n",
      "[26]\tvalid_0's binary_logloss: 0.14094\n",
      "[27]\tvalid_0's binary_logloss: 0.139509\n",
      "[28]\tvalid_0's binary_logloss: 0.138125\n",
      "[29]\tvalid_0's binary_logloss: 0.136801\n",
      "[30]\tvalid_0's binary_logloss: 0.135519\n",
      "[31]\tvalid_0's binary_logloss: 0.134296\n",
      "[32]\tvalid_0's binary_logloss: 0.133105\n",
      "[33]\tvalid_0's binary_logloss: 0.131964\n",
      "[34]\tvalid_0's binary_logloss: 0.130855\n",
      "[35]\tvalid_0's binary_logloss: 0.129793\n",
      "[36]\tvalid_0's binary_logloss: 0.128759\n",
      "[37]\tvalid_0's binary_logloss: 0.127768\n",
      "[38]\tvalid_0's binary_logloss: 0.126803\n",
      "[39]\tvalid_0's binary_logloss: 0.125877\n",
      "[40]\tvalid_0's binary_logloss: 0.124974\n",
      "[41]\tvalid_0's binary_logloss: 0.124107\n",
      "[42]\tvalid_0's binary_logloss: 0.123261\n",
      "[43]\tvalid_0's binary_logloss: 0.122449\n",
      "[44]\tvalid_0's binary_logloss: 0.121657\n",
      "[45]\tvalid_0's binary_logloss: 0.120895\n",
      "[46]\tvalid_0's binary_logloss: 0.120151\n",
      "[47]\tvalid_0's binary_logloss: 0.119436\n",
      "[48]\tvalid_0's binary_logloss: 0.118737\n",
      "[49]\tvalid_0's binary_logloss: 0.118065\n",
      "[50]\tvalid_0's binary_logloss: 0.117408\n",
      "[51]\tvalid_0's binary_logloss: 0.116776\n",
      "[52]\tvalid_0's binary_logloss: 0.116157\n",
      "[53]\tvalid_0's binary_logloss: 0.115562\n",
      "[54]\tvalid_0's binary_logloss: 0.11498\n",
      "[55]\tvalid_0's binary_logloss: 0.11442\n",
      "[56]\tvalid_0's binary_logloss: 0.113871\n",
      "[57]\tvalid_0's binary_logloss: 0.113343\n",
      "[58]\tvalid_0's binary_logloss: 0.112825\n",
      "[59]\tvalid_0's binary_logloss: 0.112328\n",
      "[60]\tvalid_0's binary_logloss: 0.11184\n",
      "[61]\tvalid_0's binary_logloss: 0.11137\n",
      "[62]\tvalid_0's binary_logloss: 0.110909\n",
      "[63]\tvalid_0's binary_logloss: 0.110466\n",
      "[64]\tvalid_0's binary_logloss: 0.110031\n",
      "[65]\tvalid_0's binary_logloss: 0.109613\n",
      "[66]\tvalid_0's binary_logloss: 0.109202\n",
      "[67]\tvalid_0's binary_logloss: 0.108807\n",
      "[68]\tvalid_0's binary_logloss: 0.108418\n",
      "[69]\tvalid_0's binary_logloss: 0.108045\n",
      "[70]\tvalid_0's binary_logloss: 0.107678\n",
      "[71]\tvalid_0's binary_logloss: 0.107325\n",
      "[72]\tvalid_0's binary_logloss: 0.106978\n",
      "[73]\tvalid_0's binary_logloss: 0.106644\n",
      "[74]\tvalid_0's binary_logloss: 0.106316\n",
      "[75]\tvalid_0's binary_logloss: 0.106\n",
      "[76]\tvalid_0's binary_logloss: 0.10569\n",
      "[77]\tvalid_0's binary_logloss: 0.105391\n",
      "[78]\tvalid_0's binary_logloss: 0.105097\n",
      "[79]\tvalid_0's binary_logloss: 0.104814\n",
      "[80]\tvalid_0's binary_logloss: 0.104536\n",
      "[81]\tvalid_0's binary_logloss: 0.104269\n",
      "[82]\tvalid_0's binary_logloss: 0.104005\n",
      "[83]\tvalid_0's binary_logloss: 0.103752\n",
      "[84]\tvalid_0's binary_logloss: 0.103502\n",
      "[85]\tvalid_0's binary_logloss: 0.103263\n",
      "[86]\tvalid_0's binary_logloss: 0.103026\n",
      "[87]\tvalid_0's binary_logloss: 0.102799\n",
      "[88]\tvalid_0's binary_logloss: 0.102575\n",
      "[89]\tvalid_0's binary_logloss: 0.10236\n",
      "[90]\tvalid_0's binary_logloss: 0.102148\n",
      "[91]\tvalid_0's binary_logloss: 0.101944\n",
      "[92]\tvalid_0's binary_logloss: 0.101743\n",
      "[93]\tvalid_0's binary_logloss: 0.10155\n",
      "[94]\tvalid_0's binary_logloss: 0.10136\n",
      "[95]\tvalid_0's binary_logloss: 0.101177\n",
      "[96]\tvalid_0's binary_logloss: 0.100996\n",
      "[97]\tvalid_0's binary_logloss: 0.100823\n",
      "[98]\tvalid_0's binary_logloss: 0.100652\n",
      "[99]\tvalid_0's binary_logloss: 0.100488\n",
      "[100]\tvalid_0's binary_logloss: 0.100325\n",
      "[101]\tvalid_0's binary_logloss: 0.10017\n",
      "[102]\tvalid_0's binary_logloss: 0.100016\n",
      "[103]\tvalid_0's binary_logloss: 0.0998681\n",
      "[104]\tvalid_0's binary_logloss: 0.0997222\n",
      "[105]\tvalid_0's binary_logloss: 0.0995822\n",
      "[106]\tvalid_0's binary_logloss: 0.099444\n",
      "[107]\tvalid_0's binary_logloss: 0.0993113\n",
      "[108]\tvalid_0's binary_logloss: 0.0991801\n",
      "[109]\tvalid_0's binary_logloss: 0.0990542\n",
      "[110]\tvalid_0's binary_logloss: 0.0989299\n",
      "[111]\tvalid_0's binary_logloss: 0.0988105\n",
      "[112]\tvalid_0's binary_logloss: 0.0986925\n",
      "[113]\tvalid_0's binary_logloss: 0.0985793\n",
      "[114]\tvalid_0's binary_logloss: 0.0984675\n",
      "[115]\tvalid_0's binary_logloss: 0.0983602\n",
      "[116]\tvalid_0's binary_logloss: 0.0982541\n",
      "[117]\tvalid_0's binary_logloss: 0.0981524\n",
      "[118]\tvalid_0's binary_logloss: 0.0980517\n",
      "[119]\tvalid_0's binary_logloss: 0.0979552\n",
      "[120]\tvalid_0's binary_logloss: 0.0978597\n",
      "[121]\tvalid_0's binary_logloss: 0.0977682\n",
      "[122]\tvalid_0's binary_logloss: 0.0976777\n",
      "[123]\tvalid_0's binary_logloss: 0.0975909\n",
      "[124]\tvalid_0's binary_logloss: 0.0975051\n",
      "[125]\tvalid_0's binary_logloss: 0.0974228\n",
      "[126]\tvalid_0's binary_logloss: 0.0973414\n",
      "[127]\tvalid_0's binary_logloss: 0.0972634\n",
      "[128]\tvalid_0's binary_logloss: 0.0971861\n",
      "[129]\tvalid_0's binary_logloss: 0.0971121\n",
      "[130]\tvalid_0's binary_logloss: 0.0970388\n",
      "[131]\tvalid_0's binary_logloss: 0.0969685\n",
      "[132]\tvalid_0's binary_logloss: 0.096899\n",
      "[133]\tvalid_0's binary_logloss: 0.0968324\n",
      "[134]\tvalid_0's binary_logloss: 0.0967664\n",
      "[135]\tvalid_0's binary_logloss: 0.0967032\n",
      "[136]\tvalid_0's binary_logloss: 0.0966407\n",
      "[137]\tvalid_0's binary_logloss: 0.0965808\n",
      "[138]\tvalid_0's binary_logloss: 0.0965215\n",
      "[139]\tvalid_0's binary_logloss: 0.0964647\n",
      "[140]\tvalid_0's binary_logloss: 0.0964084\n",
      "[141]\tvalid_0's binary_logloss: 0.0963545\n",
      "[142]\tvalid_0's binary_logloss: 0.0963011\n",
      "[143]\tvalid_0's binary_logloss: 0.09625\n",
      "[144]\tvalid_0's binary_logloss: 0.0961994\n",
      "[145]\tvalid_0's binary_logloss: 0.0961509\n",
      "[146]\tvalid_0's binary_logloss: 0.0961029\n",
      "[147]\tvalid_0's binary_logloss: 0.0960569\n",
      "[148]\tvalid_0's binary_logloss: 0.0960114\n",
      "[149]\tvalid_0's binary_logloss: 0.0959678\n",
      "[150]\tvalid_0's binary_logloss: 0.0959246\n",
      "[151]\tvalid_0's binary_logloss: 0.0958833\n",
      "[152]\tvalid_0's binary_logloss: 0.0958423\n",
      "[153]\tvalid_0's binary_logloss: 0.0958031\n",
      "[154]\tvalid_0's binary_logloss: 0.0957643\n",
      "[155]\tvalid_0's binary_logloss: 0.0957271\n",
      "[156]\tvalid_0's binary_logloss: 0.0956902\n",
      "[157]\tvalid_0's binary_logloss: 0.095655\n",
      "[158]\tvalid_0's binary_logloss: 0.0956201\n",
      "[159]\tvalid_0's binary_logloss: 0.0955867\n",
      "[160]\tvalid_0's binary_logloss: 0.0955536\n",
      "[161]\tvalid_0's binary_logloss: 0.0955219\n",
      "[162]\tvalid_0's binary_logloss: 0.0954905\n",
      "[163]\tvalid_0's binary_logloss: 0.0954605\n",
      "[164]\tvalid_0's binary_logloss: 0.0954307\n",
      "[165]\tvalid_0's binary_logloss: 0.0954023\n",
      "[166]\tvalid_0's binary_logloss: 0.0953741\n",
      "[167]\tvalid_0's binary_logloss: 0.0953472\n",
      "[168]\tvalid_0's binary_logloss: 0.0953204\n",
      "[169]\tvalid_0's binary_logloss: 0.0952949\n",
      "[170]\tvalid_0's binary_logloss: 0.0952695\n",
      "[171]\tvalid_0's binary_logloss: 0.0952454\n",
      "[172]\tvalid_0's binary_logloss: 0.0952214\n",
      "[173]\tvalid_0's binary_logloss: 0.0951985\n",
      "[174]\tvalid_0's binary_logloss: 0.0951758\n",
      "[175]\tvalid_0's binary_logloss: 0.0951541\n",
      "[176]\tvalid_0's binary_logloss: 0.0951325\n",
      "[177]\tvalid_0's binary_logloss: 0.095112\n",
      "[178]\tvalid_0's binary_logloss: 0.0950916\n",
      "[179]\tvalid_0's binary_logloss: 0.0950721\n",
      "[180]\tvalid_0's binary_logloss: 0.0950528\n",
      "[181]\tvalid_0's binary_logloss: 0.0950345\n",
      "[182]\tvalid_0's binary_logloss: 0.0950162\n",
      "[183]\tvalid_0's binary_logloss: 0.0949988\n",
      "[184]\tvalid_0's binary_logloss: 0.0949816\n",
      "[185]\tvalid_0's binary_logloss: 0.0949651\n",
      "[186]\tvalid_0's binary_logloss: 0.0949487\n",
      "[187]\tvalid_0's binary_logloss: 0.0949331\n",
      "[188]\tvalid_0's binary_logloss: 0.0949177\n",
      "[189]\tvalid_0's binary_logloss: 0.0949029\n",
      "[190]\tvalid_0's binary_logloss: 0.0948882\n",
      "[191]\tvalid_0's binary_logloss: 0.0948743\n",
      "[192]\tvalid_0's binary_logloss: 0.0948604\n",
      "[193]\tvalid_0's binary_logloss: 0.0948472\n",
      "[194]\tvalid_0's binary_logloss: 0.0948342\n",
      "[195]\tvalid_0's binary_logloss: 0.0948216\n",
      "[196]\tvalid_0's binary_logloss: 0.0948093\n",
      "[197]\tvalid_0's binary_logloss: 0.0947973\n",
      "[198]\tvalid_0's binary_logloss: 0.0947858\n",
      "[199]\tvalid_0's binary_logloss: 0.0947746\n",
      "[200]\tvalid_0's binary_logloss: 0.0947636\n",
      "[201]\tvalid_0's binary_logloss: 0.0947529\n",
      "[202]\tvalid_0's binary_logloss: 0.0947426\n",
      "[203]\tvalid_0's binary_logloss: 0.0947326\n",
      "[204]\tvalid_0's binary_logloss: 0.0947227\n",
      "[205]\tvalid_0's binary_logloss: 0.0947133\n",
      "[206]\tvalid_0's binary_logloss: 0.0947041\n",
      "[207]\tvalid_0's binary_logloss: 0.0946952\n",
      "[208]\tvalid_0's binary_logloss: 0.0946865\n",
      "[209]\tvalid_0's binary_logloss: 0.094678\n",
      "[210]\tvalid_0's binary_logloss: 0.0946698\n",
      "[211]\tvalid_0's binary_logloss: 0.0946618\n",
      "[212]\tvalid_0's binary_logloss: 0.0946542\n",
      "[213]\tvalid_0's binary_logloss: 0.0946465\n",
      "[214]\tvalid_0's binary_logloss: 0.0946393\n",
      "[215]\tvalid_0's binary_logloss: 0.0946323\n",
      "[216]\tvalid_0's binary_logloss: 0.0946254\n",
      "[217]\tvalid_0's binary_logloss: 0.0946187\n",
      "[218]\tvalid_0's binary_logloss: 0.0946122\n",
      "[219]\tvalid_0's binary_logloss: 0.094606\n",
      "[220]\tvalid_0's binary_logloss: 0.0945999\n",
      "[221]\tvalid_0's binary_logloss: 0.094594\n",
      "[222]\tvalid_0's binary_logloss: 0.0945882\n",
      "[223]\tvalid_0's binary_logloss: 0.0945827\n",
      "[224]\tvalid_0's binary_logloss: 0.0945772\n",
      "[225]\tvalid_0's binary_logloss: 0.094572\n",
      "[226]\tvalid_0's binary_logloss: 0.0945669\n",
      "[227]\tvalid_0's binary_logloss: 0.094562\n",
      "[228]\tvalid_0's binary_logloss: 0.0945572\n",
      "[229]\tvalid_0's binary_logloss: 0.0945525\n",
      "[230]\tvalid_0's binary_logloss: 0.0945481\n",
      "[231]\tvalid_0's binary_logloss: 0.0945437\n",
      "[232]\tvalid_0's binary_logloss: 0.0945396\n",
      "[233]\tvalid_0's binary_logloss: 0.0945354\n",
      "[234]\tvalid_0's binary_logloss: 0.0945315\n",
      "[235]\tvalid_0's binary_logloss: 0.0945275\n",
      "[236]\tvalid_0's binary_logloss: 0.0945239\n",
      "[237]\tvalid_0's binary_logloss: 0.0945203\n",
      "[238]\tvalid_0's binary_logloss: 0.0945169\n",
      "[239]\tvalid_0's binary_logloss: 0.0945135\n",
      "[240]\tvalid_0's binary_logloss: 0.0945103\n",
      "[241]\tvalid_0's binary_logloss: 0.094507\n",
      "[242]\tvalid_0's binary_logloss: 0.094504\n",
      "[243]\tvalid_0's binary_logloss: 0.094501\n",
      "[244]\tvalid_0's binary_logloss: 0.0944983\n",
      "[245]\tvalid_0's binary_logloss: 0.0944954\n",
      "[246]\tvalid_0's binary_logloss: 0.0944928\n",
      "[247]\tvalid_0's binary_logloss: 0.0944899\n",
      "[248]\tvalid_0's binary_logloss: 0.0944874\n",
      "[249]\tvalid_0's binary_logloss: 0.094485\n",
      "[250]\tvalid_0's binary_logloss: 0.0944827\n",
      "[251]\tvalid_0's binary_logloss: 0.0944802\n",
      "[252]\tvalid_0's binary_logloss: 0.0944782\n",
      "[253]\tvalid_0's binary_logloss: 0.0944759\n",
      "[254]\tvalid_0's binary_logloss: 0.0944739\n",
      "[255]\tvalid_0's binary_logloss: 0.0944718\n",
      "[256]\tvalid_0's binary_logloss: 0.09447\n",
      "[257]\tvalid_0's binary_logloss: 0.0944679\n",
      "[258]\tvalid_0's binary_logloss: 0.0944664\n",
      "[259]\tvalid_0's binary_logloss: 0.0944645\n",
      "[260]\tvalid_0's binary_logloss: 0.0944629\n",
      "[261]\tvalid_0's binary_logloss: 0.0944612\n",
      "[262]\tvalid_0's binary_logloss: 0.0944596\n",
      "[263]\tvalid_0's binary_logloss: 0.0944579\n",
      "[264]\tvalid_0's binary_logloss: 0.0944566\n",
      "[265]\tvalid_0's binary_logloss: 0.0944551\n",
      "[266]\tvalid_0's binary_logloss: 0.0944538\n",
      "[267]\tvalid_0's binary_logloss: 0.0944526\n",
      "[268]\tvalid_0's binary_logloss: 0.0944513\n",
      "[269]\tvalid_0's binary_logloss: 0.09445\n",
      "[270]\tvalid_0's binary_logloss: 0.0944489\n",
      "[271]\tvalid_0's binary_logloss: 0.0944477\n",
      "[272]\tvalid_0's binary_logloss: 0.0944467\n",
      "[273]\tvalid_0's binary_logloss: 0.0944456\n",
      "[274]\tvalid_0's binary_logloss: 0.0944446\n",
      "[275]\tvalid_0's binary_logloss: 0.0944435\n",
      "[276]\tvalid_0's binary_logloss: 0.0944428\n",
      "[277]\tvalid_0's binary_logloss: 0.0944419\n",
      "[278]\tvalid_0's binary_logloss: 0.094441\n",
      "[279]\tvalid_0's binary_logloss: 0.0944402\n",
      "[280]\tvalid_0's binary_logloss: 0.0944396\n",
      "[281]\tvalid_0's binary_logloss: 0.0944387\n",
      "[282]\tvalid_0's binary_logloss: 0.0944382\n",
      "[283]\tvalid_0's binary_logloss: 0.0944375\n",
      "[284]\tvalid_0's binary_logloss: 0.094437\n",
      "[285]\tvalid_0's binary_logloss: 0.0944363\n",
      "[286]\tvalid_0's binary_logloss: 0.0944358\n",
      "[287]\tvalid_0's binary_logloss: 0.0944351\n",
      "[288]\tvalid_0's binary_logloss: 0.0944348\n",
      "[289]\tvalid_0's binary_logloss: 0.0944343\n",
      "[290]\tvalid_0's binary_logloss: 0.0944339\n",
      "[291]\tvalid_0's binary_logloss: 0.0944334\n",
      "[292]\tvalid_0's binary_logloss: 0.0944332\n",
      "[293]\tvalid_0's binary_logloss: 0.0944327\n",
      "[294]\tvalid_0's binary_logloss: 0.0944325\n",
      "[295]\tvalid_0's binary_logloss: 0.0944321\n",
      "[296]\tvalid_0's binary_logloss: 0.094432\n",
      "[297]\tvalid_0's binary_logloss: 0.0944317\n",
      "[298]\tvalid_0's binary_logloss: 0.0944317\n",
      "[299]\tvalid_0's binary_logloss: 0.0944315\n",
      "[300]\tvalid_0's binary_logloss: 0.0944314\n",
      "[301]\tvalid_0's binary_logloss: 0.0944311\n",
      "[302]\tvalid_0's binary_logloss: 0.094431\n",
      "[303]\tvalid_0's binary_logloss: 0.0944309\n",
      "[304]\tvalid_0's binary_logloss: 0.0944309\n",
      "[305]\tvalid_0's binary_logloss: 0.0944308\n",
      "[306]\tvalid_0's binary_logloss: 0.0944308\n",
      "[307]\tvalid_0's binary_logloss: 0.0944307\n",
      "[308]\tvalid_0's binary_logloss: 0.0944307\n",
      "[309]\tvalid_0's binary_logloss: 0.0944306\n",
      "[310]\tvalid_0's binary_logloss: 0.0944306\n",
      "[311]\tvalid_0's binary_logloss: 0.0944305\n",
      "[312]\tvalid_0's binary_logloss: 0.0944306\n",
      "[313]\tvalid_0's binary_logloss: 0.0944305\n",
      "[314]\tvalid_0's binary_logloss: 0.0944309\n",
      "[315]\tvalid_0's binary_logloss: 0.0944308\n",
      "[316]\tvalid_0's binary_logloss: 0.0944312\n",
      "[317]\tvalid_0's binary_logloss: 0.0944311\n",
      "[318]\tvalid_0's binary_logloss: 0.0944315\n",
      "[319]\tvalid_0's binary_logloss: 0.0944315\n",
      "[320]\tvalid_0's binary_logloss: 0.0944319\n",
      "[321]\tvalid_0's binary_logloss: 0.0944319\n",
      "[322]\tvalid_0's binary_logloss: 0.0944323\n",
      "[323]\tvalid_0's binary_logloss: 0.0944324\n",
      "[324]\tvalid_0's binary_logloss: 0.0944329\n",
      "[325]\tvalid_0's binary_logloss: 0.0944329\n",
      "[326]\tvalid_0's binary_logloss: 0.0944334\n",
      "[327]\tvalid_0's binary_logloss: 0.0944335\n",
      "[328]\tvalid_0's binary_logloss: 0.0944339\n",
      "[329]\tvalid_0's binary_logloss: 0.094434\n",
      "[330]\tvalid_0's binary_logloss: 0.0944345\n",
      "[331]\tvalid_0's binary_logloss: 0.0944345\n",
      "[332]\tvalid_0's binary_logloss: 0.094435\n",
      "[333]\tvalid_0's binary_logloss: 0.0944352\n",
      "Early stopping, best iteration is:\n",
      "[313]\tvalid_0's binary_logloss: 0.0944305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f194ef79de34e4481621dc1c89b7559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best threshold is 0.3500 with F1 score: 0.6940\n",
      "2nd prediction \t time=222.86s\n"
     ]
    }
   ],
   "source": [
    "n_splits1 = 5\n",
    "prediction = stacking_level2(dataset_blend_train, dataset_blend_test,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d2187d4bbf48350eaf365b6dd8b027d5be69e0c"
   },
   "source": [
    "### Find final Thresshold\n",
    "\n",
    "Borrowed from: https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "d17dd7b0a92ec98134bf8996fd210edaadf7bed6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = df_test[['qid']].copy()\n",
    "submission['prediction'] = prediction\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "a1a07a65e47b5dcefaac040f3f633dc077e8f61e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid,prediction\r\n",
      "0000163e3ea7c7a74cd7,1\r\n",
      "00002bd4fb5d505b9161,0\r\n",
      "00007756b4a147d2b0b3,0\r\n",
      "000086e4b7e1c7146103,0\r\n",
      "0000c4c3fbe8785a3090,0\r\n",
      "000101884c19f3515c1a,0\r\n",
      "00010f62537781f44a47,0\r\n",
      "00012afbd27452239059,0\r\n",
      "00014894849d00ba98a9,0\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "f96ee914ccedc6c0218b48efcd6fc36fdc1eda22"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "26ebb5d5c2e2974fbb0f6359d0a8dae9c23e1cc8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
